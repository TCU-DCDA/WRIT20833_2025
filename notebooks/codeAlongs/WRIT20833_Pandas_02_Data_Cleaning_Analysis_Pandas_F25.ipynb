{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f75b3ae8",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/TCU-DCDA/WRIT20833-2025/blob/main/notebooks/codeAlongs/WRIT20833_Data_Cleaning_Analysis_Pandas_F25.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e8f2dc",
   "metadata": {},
   "source": [
    "# Data Cleaning & Analysis with Pandas\n",
    "## From Messy Cultural Data to Meaningful Insights\n",
    "\n",
    "Welcome to the world of **real-world cultural data** - which is almost always messy, inconsistent, and requires significant cleaning before analysis. Today we'll learn advanced pandas techniques that transform chaotic datasets into clean, analyzable information.\n",
    "\n",
    "In actual cultural research, data rarely comes pre-packaged and ready for analysis. Historical records have spelling variations, contemporary datasets have missing information, and scraped data contains formatting inconsistencies. This lesson focuses on the essential skills of **data cleaning** and **advanced analysis** that turn messy cultural materials into research-ready datasets.\n",
    "\n",
    "### üìö How This Lesson Works:\n",
    "This is an **advanced demonstration notebook** that builds on pandas fundamentals. We'll work through real examples of messy cultural data and cleaning techniques.\n",
    "\n",
    "**üéØ Ready to practice with your own messy data?** After this lesson, use the companion **Student Practice Notebook**:\n",
    "- **üìù `WRIT20833_Data_Cleaning_Student_Practice_F25.ipynb`** (in the homework folder)\n",
    "- Apply these cleaning techniques to your own cultural dataset\n",
    "- Work through guided exercises with real data challenges\n",
    "- Submit your cleaned analysis for assessment\n",
    "\n",
    "### What We'll Learn Today:\n",
    "- **Handling Missing Data**: Strategies for incomplete cultural records\n",
    "- **Text Cleaning**: Standardizing names, places, and categories using pandas string methods\n",
    "- **Data Transformation**: Reshaping and reorganizing cultural datasets\n",
    "- **Grouping & Aggregation**: Comparing patterns across categories and time periods\n",
    "- **Advanced Filtering**: Complex queries for sophisticated cultural analysis\n",
    "\n",
    "Think of today's lesson as becoming cultural data archaeologists - carefully cleaning and reconstructing fragmented information to reveal hidden patterns in cultural history.\n",
    "\n",
    "### ‚ö†Ô∏è Important: Dataset Requirements for This Lesson\n",
    "**This advanced lesson works best with datasets that have:**\n",
    "- **Rich string/text data**: Author names, titles, genres, locations, descriptions\n",
    "- **Categorical columns**: Classifications that can be standardized and grouped\n",
    "- **Numeric data**: Values for calculations, aggregations, and mathematical analysis\n",
    "- **Mixed data types**: A healthy combination of text and numbers\n",
    "\n",
    "**‚ö†Ô∏è Limitations to Consider:**\n",
    "- **Text-only datasets**: Grouping and aggregation will be limited without numeric columns\n",
    "- **Pure numeric datasets**: String cleaning methods won't be applicable\\n\",\n",
    "    \"- **Very small datasets**: Statistical patterns may not be meaningful\\n\",\n",
    "    \"- **Highly structured data**: May not need the extensive cleaning we'll practice\\n\",\n",
    "    \"\\n\",\n",
    "    \"**Before using your own data**, check that it includes a mix of messy text and numeric values. If your dataset is primarily text-based, focus on the string cleaning sections. If it's primarily numeric, focus on the grouping and aggregation techniques.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af27294e",
   "metadata": {},
   "source": [
    "## Part 1: Setting Up Our Messy Cultural Dataset\n",
    "\n",
    "Let's start with a realistic scenario: you've found a dataset of historical literary publications, but it's messy and inconsistent - just like real cultural data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0991599",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Set display options\n",
    "pd.options.display.max_rows = 100\n",
    "pd.options.display.max_columns = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621faac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a realistically messy cultural dataset\n",
    "# This simulates the kind of inconsistent data you'd find in historical records\n",
    "\n",
    "messy_cultural_data = {\n",
    "    'title': ['pride and prejudice', 'JANE EYRE', 'Wuthering Heights', 'emma', 'Frankenstein', \n",
    "              'dracula', 'The Picture of Dorian Gray', 'the importance of being earnest', \n",
    "              'The Strange Case of Dr. Jekyll and Mr. Hyde', 'TREASURE ISLAND',\n",
    "              'Alice\\'s Adventures in Wonderland', 'through the looking glass', \n",
    "              'The Time Machine', 'the war of the worlds', 'The Invisible Man'],\n",
    "    'author_name': ['Jane Austen', 'charlotte bronte', 'Emily Bront√´', 'jane austen', 'mary shelley',\n",
    "                   'Bram Stoker', 'Oscar Wilde', 'oscar wilde', 'Robert Louis Stevenson', 'r.l. stevenson',\n",
    "                   'Lewis Carroll', 'lewis carroll', 'H.G. Wells', 'h.g. wells', 'H.G. Wells'],\n",
    "    'publication_year': [1813, 1847, 1847, 1815, 1818, 1897, 1890, 1895, 1886, 1883, \n",
    "                        1865, 1871, 1895, 1898, 1897],\n",
    "    'genre': ['Romance', 'gothic', 'Gothic', 'romance', 'Science Fiction', 'Horror', \n",
    "             'Philosophical Fiction', 'Comedy', 'Horror', 'adventure', 'Fantasy', \n",
    "             'fantasy', 'science fiction', 'Science Fiction', 'sci-fi'],\n",
    "    'pages': [432, None, 416, 474, 280, 418, 254, None, 144, 292, 200, 228, 104, 192, 153],\n",
    "    'setting_country': ['England', 'england', 'England', 'England', 'Switzerland/Germany', \n",
    "                       'Romania/England', 'England', 'England', 'Scotland', 'Treasure Island',\n",
    "                       'Wonderland', 'Wonderland', 'England', 'England', 'England'],\n",
    "    'female_protagonist': ['Yes', 'yes', 'Yes', 'Yes', 'No', 'No', 'No', 'No', 'No', 'No',\n",
    "                          'Yes', 'Yes', 'No', 'No', 'No'],\n",
    "    'modern_adaptations': [15, 12, 8, 6, 25, 35, 5, 3, 18, 12, 20, 4, 15, 8, 10]\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "books_df = pd.DataFrame(messy_cultural_data)\n",
    "\n",
    "print(\"üìö Created messy literary dataset!\")\n",
    "print(f\"Dataset contains {len(books_df)} classic literary works\")\n",
    "books_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf828cb",
   "metadata": {},
   "source": [
    "### üîç Identifying Data Problems\n",
    "\n",
    "Let's examine what makes this dataset \"messy\" - these are common issues in real cultural data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9682beb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing data\n",
    "print(\"Missing data summary:\")\n",
    "print(books_df.isnull().sum())\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Look at inconsistent formatting\n",
    "print(\"Unique genres (notice inconsistencies):\")\n",
    "print(books_df['genre'].unique())\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "print(\"Unique author names (notice duplicates):\")\n",
    "print(books_df['author_name'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae93a23",
   "metadata": {},
   "source": [
    "## Part 2: Handling Missing Data\n",
    "\n",
    "Missing data is common in cultural datasets. Let's learn strategies for dealing with it responsibly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ba5837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify rows with missing data\n",
    "print(\"Rows with missing page data:\")\n",
    "missing_pages = books_df[books_df['pages'].isnull()]\n",
    "print(missing_pages[['title', 'author_name', 'pages']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2847a347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 1: Fill missing values with meaningful estimates\n",
    "# For pages, we could use the median page count for similar genres\n",
    "\n",
    "# Calculate median pages by genre (excluding missing values)\n",
    "genre_medians = books_df.groupby('genre')['pages'].median()\n",
    "print(\"Median pages by genre:\")\n",
    "print(genre_medians)\n",
    "\n",
    "# Create a copy for our cleaned data\n",
    "books_cleaned = books_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aeff1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing pages with overall median (simple approach)\n",
    "overall_median_pages = books_df['pages'].median()\n",
    "books_cleaned['pages'] = books_cleaned['pages'].fillna(overall_median_pages)\n",
    "\n",
    "print(f\"Filled missing page counts with median: {overall_median_pages}\")\n",
    "print(\"\\nChecking our work:\")\n",
    "print(books_cleaned[['title', 'pages']].loc[books_df['pages'].isnull()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87ac2a1",
   "metadata": {},
   "source": [
    "### ü§î Discussion: Ethics of Filling Missing Data\n",
    "\n",
    "When we \"fill\" missing cultural data, we're making assumptions. Consider:\n",
    "- Is it better to estimate missing values or exclude incomplete records?\n",
    "- How might our filling strategy bias our cultural analysis?\n",
    "- What does missing data itself tell us about historical record-keeping?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9443a11",
   "metadata": {},
   "source": [
    "## Part 3: Text Cleaning with Pandas String Methods\n",
    "\n",
    "Cultural data often involves text that needs standardization. Pandas string methods are powerful tools for cleaning textual cultural information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc7c1af",
   "metadata": {},
   "source": [
    "### Standardizing Book Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06352a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize title capitalization using .str.title()\n",
    "print(\"Original titles:\")\n",
    "print(books_cleaned['title'].tolist())\n",
    "\n",
    "books_cleaned['title'] = books_cleaned['title'].str.title()\n",
    "\n",
    "print(\"\\nStandardized titles:\")\n",
    "print(books_cleaned['title'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a57a937",
   "metadata": {},
   "source": [
    "### Cleaning Author Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc416891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize author names - this is trickier!\n",
    "print(\"Original author names:\")\n",
    "print(books_cleaned['author_name'].unique())\n",
    "\n",
    "# First, standardize capitalization\n",
    "books_cleaned['author_name'] = books_cleaned['author_name'].str.title()\n",
    "\n",
    "print(\"\\nAfter title case:\")\n",
    "print(books_cleaned['author_name'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232475f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle specific author name variations using .str.replace()\n",
    "# This requires domain knowledge about the authors\n",
    "\n",
    "author_corrections = {\n",
    "    'R.L. Stevenson': 'Robert Louis Stevenson',\n",
    "    'H.G. Wells': 'H.G. Wells',  # Keep this format consistent\n",
    "    'Charlotte Bronte': 'Charlotte Bront√´',  # Add proper accent\n",
    "}\n",
    "\n",
    "for old_name, new_name in author_corrections.items():\n",
    "    books_cleaned['author_name'] = books_cleaned['author_name'].str.replace(old_name, new_name)\n",
    "\n",
    "print(\"After manual corrections:\")\n",
    "print(books_cleaned['author_name'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd260f4",
   "metadata": {},
   "source": [
    "### Standardizing Genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29ee431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up genre categories\n",
    "print(\"Original genres:\")\n",
    "print(books_cleaned['genre'].unique())\n",
    "\n",
    "# Create a mapping for genre standardization\n",
    "genre_mapping = {\n",
    "    'romance': 'Romance',\n",
    "    'gothic': 'Gothic',\n",
    "    'science fiction': 'Science Fiction',\n",
    "    'sci-fi': 'Science Fiction',\n",
    "    'adventure': 'Adventure',\n",
    "    'fantasy': 'Fantasy'\n",
    "}\n",
    "\n",
    "# Apply the mapping using .str.lower() first, then .replace()\n",
    "books_cleaned['genre_clean'] = books_cleaned['genre'].str.lower()\n",
    "books_cleaned['genre_clean'] = books_cleaned['genre_clean'].replace(genre_mapping)\n",
    "books_cleaned['genre_clean'] = books_cleaned['genre_clean'].str.title()\n",
    "\n",
    "print(\"\\nCleaned genres:\")\n",
    "print(books_cleaned['genre_clean'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5427bcf5",
   "metadata": {},
   "source": [
    "### Standardizing Yes/No Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8105efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the female_protagonist column\n",
    "print(\"Original female_protagonist values:\")\n",
    "print(books_cleaned['female_protagonist'].unique())\n",
    "\n",
    "# Convert to consistent True/False values\n",
    "books_cleaned['has_female_protagonist'] = books_cleaned['female_protagonist'].str.lower() == 'yes'\n",
    "\n",
    "print(\"\\nCleaned to boolean:\")\n",
    "print(books_cleaned['has_female_protagonist'].unique())\n",
    "print(\"\\nCounts:\")\n",
    "print(books_cleaned['has_female_protagonist'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d36d34",
   "metadata": {},
   "source": [
    "## Part 4: Checking for and Handling Duplicates\n",
    "\n",
    "Cultural datasets often contain duplicate entries or near-duplicates that need identification and handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167585d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate titles\n",
    "duplicate_titles = books_cleaned['title'].duplicated()\n",
    "print(f\"Number of duplicate titles: {duplicate_titles.sum()}\")\n",
    "\n",
    "if duplicate_titles.any():\n",
    "    print(\"\\nDuplicate titles:\")\n",
    "    print(books_cleaned[duplicate_titles][['title', 'author_name']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aec3c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for books by the same author (after cleaning)\n",
    "print(\"Books per author:\")\n",
    "author_counts = books_cleaned['author_name'].value_counts()\n",
    "print(author_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5447a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find potential duplicates based on multiple columns\n",
    "potential_duplicates = books_cleaned.duplicated(subset=['title', 'author_name'], keep=False)\n",
    "print(f\"Potential duplicates based on title + author: {potential_duplicates.sum()}\")\n",
    "\n",
    "if potential_duplicates.any():\n",
    "    print(\"\\nPotential duplicate entries:\")\n",
    "    print(books_cleaned[potential_duplicates][['title', 'author_name', 'publication_year']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853a1f81",
   "metadata": {},
   "source": [
    "## Part 5: Advanced Data Analysis - Grouping and Aggregation\n",
    "\n",
    "Now that our data is clean, let's perform sophisticated cultural analysis using pandas grouping capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed43d1dd",
   "metadata": {},
   "source": [
    "### Analyzing by Author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fba658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by author and analyze their work\n",
    "author_analysis = books_cleaned.groupby('author_name').agg({\n",
    "    'title': 'count',  # Number of books\n",
    "    'publication_year': ['min', 'max'],  # Career span\n",
    "    'pages': 'mean',  # Average book length\n",
    "    'modern_adaptations': 'sum',  # Total adaptations\n",
    "    'has_female_protagonist': 'mean'  # Proportion with female protagonists\n",
    "})\n",
    "\n",
    "# Flatten column names\n",
    "author_analysis.columns = ['book_count', 'first_publication', 'last_publication', \n",
    "                          'avg_pages', 'total_adaptations', 'female_protagonist_rate']\n",
    "\n",
    "# Calculate career span\n",
    "author_analysis['career_span'] = author_analysis['last_publication'] - author_analysis['first_publication']\n",
    "\n",
    "print(\"Author Analysis:\")\n",
    "author_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43de726",
   "metadata": {},
   "source": [
    "### Analyzing by Genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7311b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by cleaned genre\n",
    "genre_analysis = books_cleaned.groupby('genre_clean').agg({\n",
    "    'title': 'count',\n",
    "    'publication_year': 'mean',\n",
    "    'pages': 'mean',\n",
    "    'modern_adaptations': 'mean',\n",
    "    'has_female_protagonist': 'mean'\n",
    "})\n",
    "\n",
    "genre_analysis.columns = ['book_count', 'avg_publication_year', 'avg_pages', \n",
    "                         'avg_adaptations', 'female_protagonist_rate']\n",
    "\n",
    "print(\"Genre Analysis:\")\n",
    "genre_analysis.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55acd48",
   "metadata": {},
   "source": [
    "### Time Period Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162f2ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create time period categories\n",
    "def categorize_period(year):\n",
    "    if year < 1850:\n",
    "        return 'Early 19th Century (pre-1850)'\n",
    "    elif year < 1880:\n",
    "        return 'Mid 19th Century (1850-1879)'\n",
    "    else:\n",
    "        return 'Late 19th Century (1880+)'\n",
    "\n",
    "books_cleaned['time_period'] = books_cleaned['publication_year'].apply(categorize_period)\n",
    "\n",
    "# Analyze by time period\n",
    "period_analysis = books_cleaned.groupby('time_period').agg({\n",
    "    'title': 'count',\n",
    "    'pages': 'mean',\n",
    "    'modern_adaptations': 'mean',\n",
    "    'has_female_protagonist': 'mean'\n",
    "})\n",
    "\n",
    "period_analysis.columns = ['book_count', 'avg_pages', 'avg_adaptations', 'female_protagonist_rate']\n",
    "\n",
    "print(\"Time Period Analysis:\")\n",
    "period_analysis.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc9df5e",
   "metadata": {},
   "source": [
    "## Part 6: Advanced Filtering and Queries\n",
    "\n",
    "Let's practice complex filtering for sophisticated cultural analysis questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dec68f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex query 1: Highly adapted works with female protagonists\n",
    "popular_female_led = books_cleaned[\n",
    "    (books_cleaned['modern_adaptations'] > 10) & \n",
    "    (books_cleaned['has_female_protagonist'] == True)\n",
    "]\n",
    "\n",
    "print(\"Highly adapted books with female protagonists:\")\n",
    "print(popular_female_led[['title', 'author_name', 'modern_adaptations', 'genre_clean']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d0e8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex query 2: Short books from prolific authors\n",
    "prolific_authors = author_analysis[author_analysis['book_count'] > 1].index\n",
    "short_books_prolific_authors = books_cleaned[\n",
    "    (books_cleaned['author_name'].isin(prolific_authors)) & \n",
    "    (books_cleaned['pages'] < 200)\n",
    "]\n",
    "\n",
    "print(\"Short books by prolific authors:\")\n",
    "print(short_books_prolific_authors[['title', 'author_name', 'pages', 'genre_clean']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e26e64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex query 3: Using string methods for advanced filtering\n",
    "# Find books with 'the' in the title\n",
    "titles_with_the = books_cleaned[books_cleaned['title'].str.lower().str.contains('the')]\n",
    "\n",
    "print(\"Books with 'The' in the title:\")\n",
    "print(titles_with_the[['title', 'author_name', 'publication_year']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b07083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex query 4: Books from authors with specific patterns in their names\n",
    "authors_with_initials = books_cleaned[books_cleaned['author_name'].str.contains(r'\\b[A-Z]\\.[A-Z]\\.')]\n",
    "\n",
    "print(\"Books by authors with initials (e.g., H.G. Wells):\")\n",
    "print(authors_with_initials[['title', 'author_name', 'genre_clean']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c534e44f",
   "metadata": {},
   "source": [
    "## Part 7: Creating Meaningful Visualizations from Clean Data\n",
    "\n",
    "Clean data enables sophisticated visualizations that reveal cultural patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3a8863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 1: Genre popularity over time\n",
    "plt.figure(figsize=(12, 6))\n",
    "for genre in books_cleaned['genre_clean'].unique():\n",
    "    genre_data = books_cleaned[books_cleaned['genre_clean'] == genre]\n",
    "    plt.scatter(genre_data['publication_year'], [genre] * len(genre_data), \n",
    "               s=genre_data['modern_adaptations'] * 3, alpha=0.7, label=genre)\n",
    "\n",
    "plt.xlabel('Publication Year')\n",
    "plt.ylabel('Genre')\n",
    "plt.title('Literary Genres Over Time\\n(bubble size = modern adaptations)')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072458a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 2: Author productivity and adaptation success\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(author_analysis['book_count'], author_analysis['total_adaptations'], \n",
    "           s=author_analysis['avg_pages'], alpha=0.7, c=author_analysis['female_protagonist_rate'], \n",
    "           cmap='RdYlBu')\n",
    "plt.xlabel('Number of Books in Dataset')\n",
    "plt.ylabel('Total Modern Adaptations')\n",
    "plt.title('Author Productivity vs. Adaptation Success\\n(bubble size = avg pages, color = female protagonist rate)')\n",
    "plt.colorbar(label='Female Protagonist Rate')\n",
    "\n",
    "# Add author labels\n",
    "for author, data in author_analysis.iterrows():\n",
    "    plt.annotate(author, (data['book_count'], data['total_adaptations']), \n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a825202c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 3: Time period comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Average pages by period\n",
    "period_analysis['avg_pages'].plot(kind='bar', ax=axes[0,0], color='skyblue')\n",
    "axes[0,0].set_title('Average Book Length by Period')\n",
    "axes[0,0].set_ylabel('Pages')\n",
    "\n",
    "# Modern adaptations by period\n",
    "period_analysis['avg_adaptations'].plot(kind='bar', ax=axes[0,1], color='lightcoral')\n",
    "axes[0,1].set_title('Average Modern Adaptations by Period')\n",
    "axes[0,1].set_ylabel('Adaptations')\n",
    "\n",
    "# Female protagonist rate by period\n",
    "period_analysis['female_protagonist_rate'].plot(kind='bar', ax=axes[1,0], color='lightgreen')\n",
    "axes[1,0].set_title('Female Protagonist Rate by Period')\n",
    "axes[1,0].set_ylabel('Rate')\n",
    "\n",
    "# Book count by period\n",
    "period_analysis['book_count'].plot(kind='bar', ax=axes[1,1], color='gold')\n",
    "axes[1,1].set_title('Number of Books by Period')\n",
    "axes[1,1].set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f930bfa",
   "metadata": {},
   "source": [
    "## Part 8: Cultural Insights from Clean Data\n",
    "\n",
    "Now that we've cleaned our literary dataset, let's explore what cultural patterns and insights emerge from proper data cleaning practices.\n",
    "\n",
    "### üéØ Key Questions Clean Data Can Help Answer:\n",
    "- **Which literary genres were most popular in different time periods?**\n",
    "- **How do publication patterns reflect cultural and historical trends?**\n",
    "- **What role does data standardization play in cultural analysis?**\n",
    "\n",
    "### üìä The Impact of Data Cleaning on Analysis\n",
    "Notice how our cleaning process has transformed messy, inconsistent data into a reliable foundation for cultural research. This is exactly the workflow you'll practice in the homework assignment with your own chosen cultural dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae483c6",
   "metadata": {},
   "source": [
    "## Part 9: Advanced String Methods for Cultural Data\n",
    "\n",
    "Let's explore more sophisticated text processing techniques that are particularly useful for cultural datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af333081",
   "metadata": {},
   "source": [
    "### Extracting Information from Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b9ffb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract publication decade\n",
    "books_cleaned['decade'] = (books_cleaned['publication_year'] // 10) * 10\n",
    "books_cleaned['decade_label'] = books_cleaned['decade'].astype(str) + 's'\n",
    "\n",
    "print(\"Decade analysis:\")\n",
    "decade_counts = books_cleaned['decade_label'].value_counts().sort_index()\n",
    "print(decade_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49568a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract word count from titles\n",
    "books_cleaned['title_word_count'] = books_cleaned['title'].str.split().str.len()\n",
    "\n",
    "print(\"Title length analysis:\")\n",
    "print(f\"Average words in title: {books_cleaned['title_word_count'].mean():.2f}\")\n",
    "print(f\"Shortest title: {books_cleaned['title_word_count'].min()} words\")\n",
    "print(f\"Longest title: {books_cleaned['title_word_count'].max()} words\")\n",
    "\n",
    "# Find the longest title\n",
    "longest_title = books_cleaned[books_cleaned['title_word_count'] == books_cleaned['title_word_count'].max()]\n",
    "print(f\"\\nLongest title: '{longest_title['title'].iloc[0]}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941e5d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for common title patterns\n",
    "print(\"Title patterns:\")\n",
    "print(f\"Titles containing 'The': {books_cleaned['title'].str.contains('The').sum()}\")\n",
    "print(f\"Titles containing 'of': {books_cleaned['title'].str.contains(' Of ').sum()}\")\n",
    "print(f\"Titles containing 'and': {books_cleaned['title'].str.contains(' And ').sum()}\")\n",
    "\n",
    "# Create a category for title types\n",
    "def categorize_title(title):\n",
    "    if title.startswith('The '):\n",
    "        return 'Starts with \"The\"'\n",
    "    elif ' Of ' in title or ' And ' in title:\n",
    "        return 'Contains \"Of\" or \"And\"'\n",
    "    else:\n",
    "        return 'Simple title'\n",
    "\n",
    "books_cleaned['title_type'] = books_cleaned['title'].apply(categorize_title)\n",
    "print(\"\\nTitle type distribution:\")\n",
    "print(books_cleaned['title_type'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dd0007",
   "metadata": {},
   "source": [
    "### Working with Complex Text Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb26423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create author nationality based on names (this would require cultural knowledge)\n",
    "def guess_author_origin(name):\n",
    "    \"\"\"This is a simplified example - real cultural data would require more sophisticated approaches\"\"\"\n",
    "    if name in ['Jane Austen', 'Charlotte Bront√´', 'Emily Bront√´']:\n",
    "        return 'English (Women Writers)'\n",
    "    elif name in ['Oscar Wilde', 'Robert Louis Stevenson']:\n",
    "        return 'British Isles (Male Writers)'\n",
    "    elif name in ['H.G. Wells', 'Lewis Carroll']:\n",
    "        return 'English (Male Writers)'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "books_cleaned['author_category'] = books_cleaned['author_name'].apply(guess_author_origin)\n",
    "\n",
    "print(\"Author categories:\")\n",
    "print(books_cleaned['author_category'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61e8a3b",
   "metadata": {},
   "source": [
    "## Part 10: Final Analysis and Insights\n",
    "\n",
    "Let's bring together all our cleaning and analysis techniques to answer sophisticated cultural questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20e88ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final comprehensive analysis\n",
    "final_analysis = books_cleaned.groupby(['decade_label', 'genre_clean']).agg({\n",
    "    'title': 'count',\n",
    "    'modern_adaptations': 'mean',\n",
    "    'has_female_protagonist': 'mean',\n",
    "    'pages': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "final_analysis.columns = ['book_count', 'avg_adaptations', 'female_protagonist_rate', 'avg_pages']\n",
    "\n",
    "print(\"Comprehensive analysis by decade and genre:\")\n",
    "print(final_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7367908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary of our cleaning work\n",
    "print(\"DATA CLEANING SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Original dataset: {len(books_df)} rows\")\n",
    "print(f\"Final cleaned dataset: {len(books_cleaned)} rows\")\n",
    "print(f\"Columns added during cleaning: {len(books_cleaned.columns) - len(books_df.columns)}\")\n",
    "print(\"\\nCleaning actions performed:\")\n",
    "print(\"‚úÖ Filled missing page data\")\n",
    "print(\"‚úÖ Standardized title capitalization\")\n",
    "print(\"‚úÖ Cleaned author names and handled variations\")\n",
    "print(\"‚úÖ Standardized genre categories\")\n",
    "print(\"‚úÖ Converted Yes/No to boolean values\")\n",
    "print(\"‚úÖ Created time period categories\")\n",
    "print(\"‚úÖ Added title analysis features\")\n",
    "print(\"‚úÖ Added author categorization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6e47dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save our cleaned dataset\n",
    "books_cleaned.to_csv('cleaned_literary_dataset.csv', index=False)\n",
    "print(\"‚úÖ Saved cleaned dataset to CSV file\")\n",
    "\n",
    "# Save our analysis results\n",
    "author_analysis.to_csv('author_analysis_results.csv')\n",
    "genre_analysis.to_csv('genre_analysis_results.csv')\n",
    "period_analysis.to_csv('period_analysis_results.csv')\n",
    "print(\"‚úÖ Saved analysis results to CSV files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2704846",
   "metadata": {},
   "source": [
    "## Cultural Insights and Discussion\n",
    "\n",
    "Based on our cleaned data analysis, let's discuss what we've learned:\n",
    "\n",
    "### Key Findings:\n",
    "1. **Gender Representation**: What patterns do we see in female protagonists across different time periods and genres?\n",
    "2. **Adaptation Patterns**: Which types of books tend to get more modern adaptations?\n",
    "3. **Historical Trends**: How did literary production change across the 19th century?\n",
    "4. **Genre Evolution**: What can we learn about how literary genres developed?\n",
    "\n",
    "### Questions for Further Research:\n",
    "- How might our cleaning choices have influenced our findings?\n",
    "- What other cultural factors might explain the patterns we observe?\n",
    "- How could we expand this analysis with additional data sources?\n",
    "- What ethical considerations arise when quantifying cultural production?\n",
    "\n",
    "*Use the space below to record your insights and discussion points.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623dc731",
   "metadata": {},
   "source": [
    "### My Cultural Analysis Insights:\n",
    "\n",
    "*Record your observations and insights here...*\n",
    "\n",
    "**Patterns I noticed:**\n",
    "\n",
    "**Questions this raises:**\n",
    "\n",
    "**Limitations of this analysis:**\n",
    "\n",
    "**Ideas for future research:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41b2640",
   "metadata": {},
   "source": [
    "## üéì Summary: From Messy to Meaningful Cultural Data\n",
    "\n",
    "Congratulations! You've learned the essential techniques for transforming messy cultural data into clean, analysis-ready datasets.\n",
    "\n",
    "### ‚úÖ Key Skills Demonstrated:\n",
    "- **Identifying data problems**: Missing values, inconsistent formatting, duplicate entries\n",
    "- **Handling missing data**: Ethical strategies for filling gaps without introducing bias\n",
    "- **Text standardization**: Using pandas string methods for consistent cultural categories\n",
    "- **Advanced analysis**: Grouping, aggregation, and trend identification\n",
    "- **Data ethics**: Understanding the cultural implications of cleaning choices\n",
    "\n",
    "### üîß Essential Pandas Techniques Mastered:\n",
    "- `df.isnull()` and `df.fillna()` for missing data handling\n",
    "- `str.title()`, `str.replace()`, and string methods for text cleaning\n",
    "- `groupby()` and aggregation functions for cultural pattern analysis\n",
    "- Creating derived variables (decades, categories, flags) for deeper insights\n",
    "- Combining multiple cleaning operations into systematic workflows\n",
    "\n",
    "### üéØ Next Steps: Apply These Skills\n",
    "**Now it's your turn!** In the companion homework assignment, you'll:\n",
    "1. **Choose your own cultural dataset** from areas that interest you\n",
    "2. **Apply these exact techniques** to clean and analyze real-world cultural data\n",
    "3. **Practice ethical data collection** principles including robots.txt compliance\n",
    "4. **Generate cultural insights** through systematic data cleaning and analysis\n",
    "\n",
    "### üöÄ Your Cultural Data Journey Continues:\n",
    "1. **Practice**: Use the homework to cement these technical skills with your chosen cultural domain\n",
    "2. **Expand**: Learn advanced pandas techniques like merging datasets and time series analysis\n",
    "3. **Specialize**: Develop expertise with tools specific to your cultural research interests\n",
    "4. **Share**: Present your cultural data findings to academic and public audiences\n",
    "\n",
    "Remember: **Clean data is the foundation of trustworthy cultural analysis.** The time you invest in careful data cleaning pays dividends in the reliability and credibility of your cultural insights!\n",
    "\n",
    "### üìö Ready for Your Own Analysis?\n",
    "Head to the **HW3-2 homework assignment** to apply these skills to your own chosen cultural dataset. You'll practice the complete workflow from data collection ethics to final cultural insights!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
