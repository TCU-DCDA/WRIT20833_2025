{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/TCU-DCDA/WRIT20833-2025/blob/main/notebooks/codeAlongs/WRIT20833_Instant_Data_Scraper_Ethics_F25.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethical Data Collection with Instant Data Scraper\n",
    "## From Web Pages to Cultural Datasets\n",
    "\n",
    "Welcome to practical data collection for cultural research! Today we'll learn to use **Instant Data Scraper**, a browser extension that makes collecting cultural data from websites accessible to humanities researchers without complex coding.\n",
    "\n",
    "### üéØ Today's Learning Goals:\n",
    "- **Install and use Instant Data Scraper** for collecting cultural data\n",
    "- **Practice ethical data collection** using robots.txt and fair use principles\n",
    "- **Clean and analyze scraped data** using pandas techniques you already know\n",
    "- **Transform web data** into research-ready datasets for cultural analysis\n",
    "\n",
    "### üìö What We'll Create:\n",
    "By the end of this lesson, you'll have:\n",
    "- A working knowledge of browser-based data collection\n",
    "- Your own scraped cultural dataset **suitable for HW4-1 and HW4-2**\n",
    "- Clean, analysis-ready data for **term frequency, sentiment analysis, and topic modeling**\n",
    "- An understanding of ethical data collection practices\n",
    "\n",
    "### üîó Connection to Your Assignments:\n",
    "The data you collect today will be perfect for:\n",
    "- **HW4-1**: Term frequency analysis and sentiment analysis with VADER\n",
    "- **HW4-2**: Topic modeling with Gensim LDA to discover hidden themes\n",
    "- **Future projects**: Building your own cultural research datasets\n",
    "\n",
    "### ‚öñÔ∏è Ethics First:\n",
    "Before we start collecting data, we'll establish ethical guidelines that respect website owners, cultural institutions, and research subjects. **Good data collection practices are essential for responsible digital humanities work.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding Ethical Data Collection\n",
    "\n",
    "### The Golden Rule of Web Scraping\n",
    "**Collect data as you would want others to collect from your website**: respectfully, legally, and with proper attribution.\n",
    "\n",
    "### Three Pillars of Ethical Data Collection\n",
    "\n",
    "#### 1. ü§ñ **robots.txt Compliance**\n",
    "Always check a website's robots.txt file before collecting data.\n",
    "\n",
    "#### 2. ‚öñÔ∏è **Legal and Fair Use**\n",
    "Collect only what you need for legitimate research purposes.\n",
    "\n",
    "#### 3. üìö **Attribution and Transparency**\n",
    "Give proper credit and be transparent about your methods.\n",
    "\n",
    "### Let's Practice: Checking robots.txt\n",
    "\n",
    "**How to check robots.txt for any website:**\n",
    "1. Take the main website URL\n",
    "2. Add `/robots.txt` to the end\n",
    "3. Visit that URL to see the rules\n",
    "\n",
    "**Examples to try:**\n",
    "- `https://www.imdb.com/robots.txt`\n",
    "- `https://archive.org/robots.txt`\n",
    "- `https://www.goodreads.com/robots.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start by importing the libraries we'll need for data analysis\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Display settings for better output\n",
    "pd.options.display.max_rows = 20\n",
    "pd.options.display.max_columns = 10\n",
    "\n",
    "print(\"‚úÖ Libraries imported and ready for cultural data analysis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Activity: Robots.txt Exploration\n",
    "\n",
    "**Instructions:**\n",
    "1. Choose 2-3 cultural websites you're interested in (museums, libraries, databases)\n",
    "2. Check their robots.txt files\n",
    "3. Record what you find in the cell below\n",
    "\n",
    "**Good cultural sites to check:**\n",
    "- Museum websites (MoMA, Smithsonian, local museums)\n",
    "- Library catalogs and digital collections\n",
    "- Cultural databases (IMDb, Goodreads, AllMusic)\n",
    "- News and media sites\n",
    "- Academic repositories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My robots.txt findings:\n",
    "\n",
    "**Website 1:** [URL]\n",
    "- What robots.txt says:\n",
    "- My interpretation:\n",
    "\n",
    "**Website 2:** [URL]\n",
    "- What robots.txt says:\n",
    "- My interpretation:\n",
    "\n",
    "**Website 3:** [URL]\n",
    "- What robots.txt says:\n",
    "- My interpretation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Installing and Setting Up Instant Data Scraper\n",
    "\n",
    "**Instant Data Scraper** is a browser extension that automates data collection from web pages without requiring coding knowledge. It's perfect for humanities researchers who want to collect cultural data efficiently.\n",
    "\n",
    "### Installation Steps:\n",
    "\n",
    "#### For Chrome Users:\n",
    "1. Go to the Chrome Web Store\n",
    "2. Search for \"Instant Data Scraper\"\n",
    "3. Click \"Add to Chrome\"\n",
    "4. Confirm the installation\n",
    "5. Look for the extension icon in your browser toolbar\n",
    "\n",
    "#### For Firefox Users:\n",
    "1. Go to Firefox Add-ons\n",
    "2. Search for \"Instant Data Scraper\"\n",
    "3. Click \"Add to Firefox\"\n",
    "4. Confirm the installation\n",
    "\n",
    "### üéØ Quick Test:\n",
    "Once installed, visit any website with structured data (like a list of articles or products) and click the extension icon. You should see options to start scraping!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Choosing Cultural Data Sources for Text Analysis\n",
    "\n",
    "### üéØ What Makes Good Data for HW4-1 and HW4-2?\n",
    "\n",
    "#### **Best Data Types for Your Assignments:**\n",
    "\n",
    "**üåü Excellent Choices:**\n",
    "- **Product reviews** (books, movies, restaurants): Rich sentiment + varied topics\n",
    "- **News article summaries**: Clear themes + diverse vocabulary\n",
    "- **Blog posts or forum discussions**: Substantial text + authentic voice\n",
    "- **Academic paper abstracts**: Consistent length + rich terminology\n",
    "- **Museum/gallery descriptions**: Cultural content + descriptive language\n",
    "\n",
    "**‚úÖ Good Choices:**\n",
    "- **Book/movie plot summaries**: Narrative content with themes\n",
    "- **Comment sections** from cultural sites: Varied opinions + informal language\n",
    "- **Grant or funding descriptions**: Purpose-driven text with clear goals\n",
    "- **Event or exhibition descriptions**: Cultural context + descriptive details\n",
    "\n",
    "**‚ö†Ô∏è Challenging for Topic Modeling:**\n",
    "- **Just titles or headlines**: Too short for meaningful topics\n",
    "- **Single-word categories**: No content for analysis\n",
    "- **Heavily technical data**: May have limited vocabulary variation\n",
    "- **Very short entries**: Less than 20-30 words per entry\n",
    "\n",
    "### Good Websites for Cultural Data Collection\n",
    "\n",
    "**‚úÖ Generally Scraper-Friendly Cultural Sites:**\n",
    "- **Public cultural databases**: Many openly share data for research\n",
    "- **Academic repositories**: Often encourage scholarly use\n",
    "- **Government cultural sites**: Public domain information\n",
    "- **Archive.org collections**: Explicitly supportive of research access\n",
    "\n",
    "**‚ö†Ô∏è Proceed with Caution:**\n",
    "- **Commercial sites**: Check terms of service carefully\n",
    "- **Social media**: May require API access instead\n",
    "- **Sites with user accounts**: Respect privacy and login requirements\n",
    "\n",
    "**‚ùå Generally Avoid:**\n",
    "- Sites that explicitly prohibit scraping in robots.txt\n",
    "- Sites requiring personal login information\n",
    "- Sites with sensitive personal data\n",
    "\n",
    "### Practice Sites for This Lesson:\n",
    "\n",
    "We'll practice with sites that are research-friendly and have text-rich cultural data:\n",
    "\n",
    "1. **Archive.org** - Book descriptions and historical documents\n",
    "2. **Public library catalogs** - Book summaries and reviews\n",
    "3. **Museum collection databases** - Artwork descriptions and curatorial notes\n",
    "4. **Cultural news sites** - Article summaries and cultural commentary\n",
    "5. **Academic databases** - Paper abstracts and research summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Hands-On Scraping Practice\n",
    "\n",
    "### Practice Session 1: Archive.org Collections\n",
    "\n",
    "**Target Site**: Archive.org book collections\n",
    "**Why it's ethical**: Archive.org explicitly supports research access\n",
    "**What we'll collect**: Book titles, authors, publication dates, subjects, **and descriptions**\n",
    "\n",
    "#### üéØ Focus on Text-Rich Data:\n",
    "For your assignments, prioritize collecting:\n",
    "- **Book descriptions or summaries** (perfect for topic modeling)\n",
    "- **Subject classifications** (useful for understanding themes)\n",
    "- **Reviews or comments** (great for sentiment analysis)\n",
    "- **Any lengthy text fields** that contain substantial content\n",
    "\n",
    "#### Step-by-Step Instructions:\n",
    "\n",
    "1. **Navigate to Archive.org**\n",
    "   - Go to `archive.org`\n",
    "   - Click on \"Books\" or \"Texts\"\n",
    "   - Choose a collection that interests you (e.g., \"American Literature\")\n",
    "   - **Look for collections with descriptions or summaries**\n",
    "\n",
    "2. **Activate Instant Data Scraper**\n",
    "   - Click the extension icon\n",
    "   - The tool will automatically detect data patterns\n",
    "   - **Prioritize text fields**: descriptions, summaries, reviews\n",
    "   - Also capture: titles, authors, dates, subjects\n",
    "\n",
    "3. **Review and Refine**\n",
    "   - Check that you're getting substantial text content\n",
    "   - Adjust selections to include description fields\n",
    "   - Aim for entries with meaningful text (not just metadata)\n",
    "\n",
    "4. **Export Data**\n",
    "   - Choose CSV format for easy pandas import\n",
    "   - Save with a descriptive name: \"archive_books_with_descriptions.csv\"\n",
    "   - Note which column contains your main text for analysis\n",
    "\n",
    "### üìù Document Your Process:\n",
    "\n",
    "**üí° Pro Tips for Better Data:**\n",
    "- Look for \"Description\" or \"Summary\" fields\n",
    "- Collections with user reviews are excellent for sentiment analysis\n",
    "- Academic collections often have rich abstracts\n",
    "- Historical collections may have detailed contextual information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My Archive.org Scraping Session:\n",
    "\n",
    "**Collection chosen:**\n",
    "\n",
    "**Number of items collected:**\n",
    "\n",
    "**Data fields captured:**\n",
    "\n",
    "**Any challenges encountered:**\n",
    "\n",
    "**File saved as:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Loading and Analyzing Your Scraped Data\n",
    "\n",
    "Now let's use pandas to analyze the cultural data you just collected!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your scraped data\n",
    "# Replace 'your_filename.csv' with the actual name of your file\n",
    "\n",
    "# If you're in Google Colab, you'll need to upload your file first:\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "\n",
    "# Load the data\n",
    "# scraped_data = pd.read_csv('your_filename.csv')\n",
    "\n",
    "# For demonstration, let's create a sample dataset\n",
    "# Replace this with your actual data loading\n",
    "\n",
    "sample_scraped_data = {\n",
    "    'title': ['The Great Gatsby', 'To Kill a Mockingbird', 'Pride and Prejudice', \n",
    "              'The Catcher in the Rye', 'Lord of the Flies', '1984', \n",
    "              'Animal Farm', 'Brave New World', 'Jane Eyre', 'Wuthering Heights'],\n",
    "    'author': ['F. Scott Fitzgerald', 'Harper Lee', 'Jane Austen', \n",
    "               'J.D. Salinger', 'William Golding', 'George Orwell', \n",
    "               'George Orwell', 'Aldous Huxley', 'Charlotte Bront√´', 'Emily Bront√´'],\n",
    "    'year': [1925, 1960, 1813, 1951, 1954, 1949, 1945, 1932, 1847, 1847],\n",
    "    'subject': ['American Literature', 'Social Issues', 'Romance', \n",
    "                'Coming of Age', 'Dystopian', 'Political Fiction', \n",
    "                'Allegory', 'Science Fiction', 'Gothic', 'Gothic'],\n",
    "    'pages': [180, 281, 432, 277, 224, 328, 112, 268, 500, 416]\n",
    "}\n",
    "\n",
    "scraped_data = pd.DataFrame(sample_scraped_data)\n",
    "\n",
    "print(\"üìö Scraped cultural data loaded successfully!\")\n",
    "print(f\"Dataset contains {len(scraped_data)} items\")\n",
    "scraped_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore your scraped data\n",
    "print(\"=== SCRAPED DATA EXPLORATION ===\")\n",
    "print(f\"Shape: {scraped_data.shape}\")\n",
    "print(f\"Columns: {scraped_data.columns.tolist()}\")\n",
    "print(\"\\n=== DATA TYPES ===\")\n",
    "print(scraped_data.dtypes)\n",
    "print(\"\\n=== MISSING VALUES ===\")\n",
    "print(scraped_data.isnull().sum())\n",
    "print(\"\\n=== SAMPLE DATA ===\")\n",
    "print(scraped_data.sample(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Your Scraped Data\n",
    "\n",
    "Scraped data often needs cleaning. Let's apply the pandas techniques you've learned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and standardize your scraped data\n",
    "scraped_cleaned = scraped_data.copy()\n",
    "\n",
    "# Example cleaning steps (adjust based on your data)\n",
    "\n",
    "# 1. Standardize text fields\n",
    "if 'title' in scraped_cleaned.columns:\n",
    "    scraped_cleaned['title'] = scraped_cleaned['title'].str.strip()  # Remove extra spaces\n",
    "    \n",
    "if 'author' in scraped_cleaned.columns:\n",
    "    scraped_cleaned['author'] = scraped_cleaned['author'].str.strip()\n",
    "\n",
    "# 2. Handle numeric fields\n",
    "if 'year' in scraped_cleaned.columns:\n",
    "    # Ensure year is numeric\n",
    "    scraped_cleaned['year'] = pd.to_numeric(scraped_cleaned['year'], errors='coerce')\n",
    "\n",
    "# 3. Create derived fields for analysis\n",
    "if 'year' in scraped_cleaned.columns:\n",
    "    # Create time periods\n",
    "    def categorize_period(year):\n",
    "        if pd.isna(year):\n",
    "            return 'Unknown'\n",
    "        elif year < 1850:\n",
    "            return 'Pre-1850'\n",
    "        elif year < 1900:\n",
    "            return '1850-1899'\n",
    "        elif year < 1950:\n",
    "            return '1900-1949'\n",
    "        else:\n",
    "            return '1950+'\n",
    "    \n",
    "    scraped_cleaned['time_period'] = scraped_cleaned['year'].apply(categorize_period)\n",
    "\n",
    "print(\"‚úÖ Data cleaning completed!\")\n",
    "print(\"\\n=== CLEANED DATA SAMPLE ===\")\n",
    "scraped_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Your Cultural Data\n",
    "\n",
    "Now let's discover patterns in your scraped cultural data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze patterns in your scraped data\n",
    "\n",
    "print(\"=== CULTURAL DATA ANALYSIS ===\")\n",
    "\n",
    "# 1. Count by categories\n",
    "if 'subject' in scraped_cleaned.columns:\n",
    "    print(\"\\nüìä Subject/Genre Distribution:\")\n",
    "    subject_counts = scraped_cleaned['subject'].value_counts()\n",
    "    print(subject_counts)\n",
    "\n",
    "if 'time_period' in scraped_cleaned.columns:\n",
    "    print(\"\\nüìÖ Time Period Distribution:\")\n",
    "    period_counts = scraped_cleaned['time_period'].value_counts()\n",
    "    print(period_counts)\n",
    "\n",
    "# 2. Author analysis\n",
    "if 'author' in scraped_cleaned.columns:\n",
    "    print(\"\\n‚úçÔ∏è Most Represented Authors:\")\n",
    "    author_counts = scraped_cleaned['author'].value_counts().head(5)\n",
    "    print(author_counts)\n",
    "\n",
    "# 3. Numeric analysis\n",
    "if 'year' in scraped_cleaned.columns:\n",
    "    print(\"\\nüìà Year Statistics:\")\n",
    "    print(f\"Earliest work: {scraped_cleaned['year'].min()}\")\n",
    "    print(f\"Latest work: {scraped_cleaned['year'].max()}\")\n",
    "    print(f\"Average year: {scraped_cleaned['year'].mean():.1f}\")\n",
    "\n",
    "if 'pages' in scraped_cleaned.columns:\n",
    "    print(\"\\nüìñ Page Statistics:\")\n",
    "    print(f\"Average pages: {scraped_cleaned['pages'].mean():.1f}\")\n",
    "    print(f\"Shortest work: {scraped_cleaned['pages'].min()} pages\")\n",
    "    print(f\"Longest work: {scraped_cleaned['pages'].max()} pages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Your Scraped Data\n",
    "\n",
    "Let's create visualizations to reveal patterns in your cultural data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations from your scraped data\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Visualization 1: Subject/Genre distribution\n",
    "if 'subject' in scraped_cleaned.columns:\n",
    "    subject_counts = scraped_cleaned['subject'].value_counts()\n",
    "    subject_counts.plot(kind='bar', ax=axes[0,0], color='skyblue')\n",
    "    axes[0,0].set_title('Distribution by Subject/Genre')\n",
    "    axes[0,0].set_xlabel('Subject')\n",
    "    axes[0,0].set_ylabel('Count')\n",
    "    axes[0,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Visualization 2: Time period distribution\n",
    "if 'time_period' in scraped_cleaned.columns:\n",
    "    period_counts = scraped_cleaned['time_period'].value_counts()\n",
    "    period_counts.plot(kind='pie', ax=axes[0,1], autopct='%1.1f%%')\n",
    "    axes[0,1].set_title('Distribution by Time Period')\n",
    "    axes[0,1].set_ylabel('')\n",
    "\n",
    "# Visualization 3: Publication timeline\n",
    "if 'year' in scraped_cleaned.columns:\n",
    "    scraped_cleaned['year'].hist(bins=15, ax=axes[1,0], color='lightcoral', alpha=0.7)\n",
    "    axes[1,0].set_title('Publication Timeline')\n",
    "    axes[1,0].set_xlabel('Year')\n",
    "    axes[1,0].set_ylabel('Number of Works')\n",
    "\n",
    "# Visualization 4: Page length distribution\n",
    "if 'pages' in scraped_cleaned.columns:\n",
    "    scraped_cleaned['pages'].hist(bins=10, ax=axes[1,1], color='lightgreen', alpha=0.7)\n",
    "    axes[1,1].set_title('Page Length Distribution')\n",
    "    axes[1,1].set_xlabel('Number of Pages')\n",
    "    axes[1,1].set_ylabel('Number of Works')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Visualizations created from your scraped cultural data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Advanced Scraping Practice\n",
    "\n",
    "### Practice Session 2: Your Choice!\n",
    "\n",
    "Now try scraping a different type of cultural data. Here are some suggestions:\n",
    "\n",
    "**Option A: Museum Collection**\n",
    "- Find a museum with an online collection database\n",
    "- Scrape artwork titles, artists, dates, mediums\n",
    "\n",
    "**Option B: Library Catalog**\n",
    "- Choose a public library's online catalog\n",
    "- Scrape book information for a specific topic\n",
    "\n",
    "**Option C: Cultural News Site**\n",
    "- Find an arts/culture news website\n",
    "- Scrape article titles, dates, and topics\n",
    "\n",
    "**Option D: Academic Database**\n",
    "- Look for an open academic repository\n",
    "- Scrape paper titles, authors, and abstracts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My Second Scraping Session:\n",
    "\n",
    "**Website chosen:**\n",
    "\n",
    "**Why this site (ethical considerations):**\n",
    "\n",
    "**Robots.txt check results:**\n",
    "\n",
    "**Data collected:**\n",
    "\n",
    "**Challenges encountered:**\n",
    "\n",
    "**Insights discovered:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and analyze your second dataset here\n",
    "# Follow the same pattern as above:\n",
    "# 1. Load the data\n",
    "# 2. Clean and standardize\n",
    "# 3. Analyze patterns\n",
    "# 4. Create visualizations\n",
    "\n",
    "# second_dataset = pd.read_csv('your_second_file.csv')\n",
    "# print(f\"Second dataset loaded: {len(second_dataset)} items\")\n",
    "# second_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Data Collection Ethics Reflection\n",
    "\n",
    "### Comparing Different Collection Methods\n",
    "\n",
    "Let's reflect on the ethical considerations of different data collection approaches:\n",
    "\n",
    "#### Browser Extensions vs. Code-Based Scraping\n",
    "\n",
    "**Browser Extensions (like Instant Data Scraper):**\n",
    "- ‚úÖ **Easier for beginners**: No complex coding required\n",
    "- ‚úÖ **Visible process**: You can see exactly what data you're collecting\n",
    "- ‚úÖ **Human-speed collection**: Less likely to overwhelm servers\n",
    "- ‚ö†Ô∏è **Limited scale**: Can't easily collect massive datasets\n",
    "- ‚ö†Ô∏è **Manual operation**: Requires human supervision\n",
    "\n",
    "**Code-Based Scraping (like Beautiful Soup):**\n",
    "- ‚úÖ **Powerful automation**: Can collect large datasets quickly\n",
    "- ‚úÖ **Precise control**: Can implement complex ethical rules\n",
    "- ‚ö†Ô∏è **Potential for abuse**: Easy to overwhelm servers or ignore robots.txt\n",
    "- ‚ö†Ô∏è **Technical complexity**: Requires significant programming knowledge\n",
    "\n",
    "### When to Use Each Approach\n",
    "\n",
    "**Use Browser Extensions when:**\n",
    "- You need a few hundred to a few thousand data points\n",
    "- You want to see exactly what you're collecting\n",
    "- You're new to data collection\n",
    "- You want to minimize technical and ethical risks\n",
    "\n",
    "**Consider Code-Based Scraping when:**\n",
    "- You need very large datasets (10,000+ items)\n",
    "- You need to collect data regularly or repeatedly\n",
    "- You have significant programming experience\n",
    "- You can implement proper ethical safeguards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü§î Ethical Decision-Making Framework\n",
    "\n",
    "Before collecting any cultural data, ask yourself:\n",
    "\n",
    "#### 1. **Legal and Permission Questions:**\n",
    "- Does robots.txt allow this collection?\n",
    "- Are there terms of service I need to respect?\n",
    "- Is this data publicly available for research?\n",
    "\n",
    "#### 2. **Scale and Impact Questions:**\n",
    "- How much data do I actually need for my research?\n",
    "- Will my collection method strain the website's servers?\n",
    "- Can I space out my requests to be respectful?\n",
    "\n",
    "#### 3. **Research Purpose Questions:**\n",
    "- Is this data collection for legitimate academic research?\n",
    "- Will I give proper attribution to the data source?\n",
    "- How will I store and protect any sensitive information?\n",
    "\n",
    "#### 4. **Cultural Sensitivity Questions:**\n",
    "- Does this data represent people or communities?\n",
    "- How might my analysis impact those represented in the data?\n",
    "- Am I considering issues of representation and bias?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Preparing Data for Research Projects\n",
    "\n",
    "Your scraped data is now ready for serious cultural analysis! Let's prepare it for use in research projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save your cleaned datasets for future use\n",
    "\n",
    "# Add metadata to document your collection process\n",
    "collection_metadata = {\n",
    "    'collection_date': pd.Timestamp.now(),\n",
    "    'source_website': 'Your website here',\n",
    "    'collection_method': 'Instant Data Scraper browser extension',\n",
    "    'robots_txt_checked': True,\n",
    "    'ethical_considerations': 'Research use, proper attribution, respect for robots.txt',\n",
    "    'total_items': len(scraped_cleaned),\n",
    "    'data_fields': scraped_cleaned.columns.tolist()\n",
    "}\n",
    "\n",
    "print(\"üìù Collection Metadata:\")\n",
    "for key, value in collection_metadata.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Save the cleaned dataset\n",
    "output_filename = 'scraped_cultural_data_cleaned.csv'\n",
    "scraped_cleaned.to_csv(output_filename, index=False)\n",
    "print(f\"\\n‚úÖ Cleaned dataset saved as: {output_filename}\")\n",
    "\n",
    "# Create a documentation file\n",
    "documentation = f\"\"\"\n",
    "CULTURAL DATA COLLECTION DOCUMENTATION\n",
    "=====================================\n",
    "\n",
    "Collection Date: {collection_metadata['collection_date']}\n",
    "Source: {collection_metadata['source_website']}\n",
    "Method: {collection_metadata['collection_method']}\n",
    "Total Items: {collection_metadata['total_items']}\n",
    "Data Fields: {', '.join(collection_metadata['data_fields'])}\n",
    "\n",
    "Ethical Considerations:\n",
    "- Robots.txt compliance: {collection_metadata['robots_txt_checked']}\n",
    "- Research purpose: Academic cultural analysis\n",
    "- Attribution: Proper credit given to data source\n",
    "- Data protection: Stored securely for research use only\n",
    "\n",
    "Data Processing:\n",
    "- Text fields standardized (stripped whitespace)\n",
    "- Numeric fields converted to appropriate types\n",
    "- Time periods categorized for analysis\n",
    "- Missing values handled appropriately\n",
    "\n",
    "Recommended Citation:\n",
    "Data collected from {collection_metadata['source_website']} on {collection_metadata['collection_date'].strftime('%Y-%m-%d')} using ethical web scraping practices for academic research.\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nüìö Data Documentation Created:\")\n",
    "print(documentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic Modeling Readiness Check\n",
    "# This helps ensure your data will work well for HW4-2\n",
    "\n",
    "# Identify your main text column for analysis\n",
    "# Replace 'text_column' with your actual column name that contains substantial text\n",
    "# For our sample data, we'll simulate using 'title' but you should use your richest text field\n",
    "\n",
    "if 'title' in scraped_cleaned.columns:\n",
    "    text_column = 'title'  # Replace this with your main text column\n",
    "    \n",
    "    print(\"üîç TOPIC MODELING READINESS CHECK\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Check 1: Number of documents\n",
    "    doc_count = len(scraped_cleaned)\n",
    "    print(f\"üìä Total documents: {doc_count}\")\n",
    "    if doc_count >= 500:\n",
    "        print(\"   ‚úÖ Good: Sufficient documents for topic modeling\")\n",
    "    elif doc_count >= 200:\n",
    "        print(\"   ‚ö†Ô∏è  Okay: May work, but more documents would be better\")\n",
    "    else:\n",
    "        print(\"   ‚ùå Warning: You may need more documents for reliable topics\")\n",
    "    \n",
    "    # Check 2: Text length analysis\n",
    "    text_lengths = scraped_cleaned[text_column].str.len()\n",
    "    avg_length = text_lengths.mean()\n",
    "    print(f\"\\nüìù Average text length: {avg_length:.1f} characters\")\n",
    "    if avg_length >= 100:\n",
    "        print(\"   ‚úÖ Good: Text entries are substantial\")\n",
    "    elif avg_length >= 50:\n",
    "        print(\"   ‚ö†Ô∏è  Okay: Text length may be sufficient\")\n",
    "    else:\n",
    "        print(\"   ‚ùå Warning: Text entries may be too short for topic modeling\")\n",
    "    \n",
    "    # Check 3: Sample text examination\n",
    "    print(f\"\\nüìñ Sample text entries:\")\n",
    "    for i in range(min(3, len(scraped_cleaned))):\n",
    "        sample_text = scraped_cleaned[text_column].iloc[i]\n",
    "        print(f\"   {i+1}. {sample_text[:100]}{'...' if len(sample_text) > 100 else ''}\")\n",
    "    \n",
    "    # Check 4: Vocabulary richness preview\n",
    "    all_text = ' '.join(scraped_cleaned[text_column].astype(str))\n",
    "    word_count = len(all_text.split())\n",
    "    unique_words = len(set(all_text.lower().split()))\n",
    "    vocab_ratio = unique_words / word_count if word_count > 0 else 0\n",
    "    \n",
    "    print(f\"\\nüî§ Vocabulary analysis:\")\n",
    "    print(f\"   Total words: {word_count:,}\")\n",
    "    print(f\"   Unique words: {unique_words:,}\")\n",
    "    print(f\"   Vocabulary richness: {vocab_ratio:.3f}\")\n",
    "    \n",
    "    if vocab_ratio >= 0.3:\n",
    "        print(\"   ‚úÖ Good: Rich vocabulary for topic discovery\")\n",
    "    elif vocab_ratio >= 0.2:\n",
    "        print(\"   ‚ö†Ô∏è  Okay: Moderate vocabulary diversity\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è  Warning: Limited vocabulary - topics may be less distinct\")\n",
    "    \n",
    "    print(\"\\nüí° RECOMMENDATIONS:\")\n",
    "    print(\"   For HW4-1: Focus on term frequency and sentiment analysis\")\n",
    "    print(\"   For HW4-2: Consider if you need additional data or different text fields\")\n",
    "    print(\"   If text is short: Look for description/summary fields when scraping\")\n",
    "    print(\"   If vocabulary is limited: Consider broader or more diverse data sources\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Replace 'title' with your actual text column name to run this check\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection Questions\n",
    "\n",
    "Take a moment to reflect on today's data collection experience:\n",
    "\n",
    "### 1. **Technical Learning:**\n",
    "- How did using Instant Data Scraper compare to manual data entry?\n",
    "- What technical challenges did you encounter, and how did you solve them?\n",
    "- Which aspects of the data cleaning process were most important for your scraped data?\n",
    "- Did you successfully collect text-rich data suitable for topic modeling?\n",
    "\n",
    "### 2. **Data Quality for Analysis:**\n",
    "- How well does your collected data meet the requirements for HW4-1 (sentiment analysis)?\n",
    "- Do you have sufficient text content for HW4-2 (topic modeling with Gensim LDA)?\n",
    "- What trade-offs did you make between data quantity and text quality?\n",
    "- What would you do differently if collecting data again?\n",
    "\n",
    "### 3. **Ethical Considerations:**\n",
    "- How did checking robots.txt influence your choice of data sources?\n",
    "- What ethical questions arose during your data collection process?\n",
    "- How might different collection methods (manual vs. automated) impact your research ethics?\n",
    "- How did you balance research needs with respect for website resources?\n",
    "\n",
    "### 4. **Research Applications:**\n",
    "- What cultural questions could you explore with the data you collected today?\n",
    "- How does having access to larger datasets change the kinds of research questions you can ask?\n",
    "- What specific topics or themes do you predict will emerge from your data in topic modeling?\n",
    "- What limitations or biases might exist in your scraped data?\n",
    "\n",
    "### 5. **Future Learning:**\n",
    "- What other types of cultural data would you like to learn to collect?\n",
    "- How might you use these data collection skills in your future research or coursework?\n",
    "- What questions do you have about more advanced data collection techniques?\n",
    "- How confident do you feel about proceeding to text analysis with your collected data?\n",
    "\n",
    "*Write your reflections in the cell below:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My Reflections on Data Collection:\n",
    "\n",
    "**Technical Learning:**\n",
    "\n",
    "**Ethical Considerations:**\n",
    "\n",
    "**Research Applications:**\n",
    "\n",
    "**Future Learning:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: From Web Pages to Cultural Insights\n",
    "\n",
    "Congratulations! You've successfully learned to collect, clean, and analyze cultural data from the web using ethical practices. \n",
    "\n",
    "### ‚úÖ Key Skills Accomplished Today:\n",
    "\n",
    "**Ethical Data Collection:**\n",
    "- ‚úÖ Checking robots.txt for permission\n",
    "- ‚úÖ Respecting website terms and server load\n",
    "- ‚úÖ Documenting collection methods transparently\n",
    "- ‚úÖ Giving proper attribution to data sources\n",
    "\n",
    "**Technical Data Skills:**\n",
    "- ‚úÖ Installing and using Instant Data Scraper\n",
    "- ‚úÖ Collecting structured data from cultural websites\n",
    "- ‚úÖ Cleaning and standardizing scraped data with pandas\n",
    "- ‚úÖ Creating visualizations from web-collected data\n",
    "\n",
    "**Research Preparation:**\n",
    "- ‚úÖ Preparing datasets for cultural analysis projects\n",
    "- ‚úÖ Creating proper documentation for research use\n",
    "- ‚úÖ Understanding limitations and biases in scraped data\n",
    "- ‚úÖ Connecting data collection to research questions\n",
    "\n",
    "### üöÄ Next Steps in Your Digital Humanities Journey:\n",
    "\n",
    "**Immediate Applications:**\n",
    "- Use scraped data for text analysis assignments (HW4-1 style projects)\n",
    "- Apply these collection methods to research topics that interest you\n",
    "- Practice ethical data collection with different types of cultural sites\n",
    "\n",
    "**Advanced Learning:**\n",
    "- Learn about APIs for accessing cultural databases\n",
    "- Explore more sophisticated text analysis techniques\n",
    "- Study digital humanities research methodologies\n",
    "\n",
    "**Critical Perspective:**\n",
    "- Always consider the ethical implications of your data collection\n",
    "- Think critically about representation and bias in cultural datasets\n",
    "- Understand the limitations of automated data collection methods\n",
    "\n",
    "### üí° Remember:\n",
    "**Good data collection is the foundation of trustworthy cultural research.** The ethical practices and technical skills you've learned today will serve you throughout your digital humanities work. Always prioritize respect, transparency, and proper attribution in your research methods.\n",
    "\n",
    "**You're now equipped to collect cultural data responsibly and analyze it meaningfully** - essential skills for 21st-century humanities scholarship!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
