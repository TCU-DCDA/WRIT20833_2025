{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/TCU-DCDA/WRIT20833-2025/blob/main/notebooks/codeAlongs/WRIT20833_Topic_Modeling_Gensim_F25.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling with Gensim LDA\n",
    "## Discovering Hidden Themes in Cultural Texts\n",
    "\n",
    "Welcome to topic modeling! Today we'll learn to uncover **hidden themes and topics** in large collections of text using machine learning.\n",
    "\n",
    "### üéØ What You'll Learn:\n",
    "- **Understand topic modeling** as a distant reading technique\n",
    "- **Install and use Gensim LDA** for discovering hidden topics\n",
    "- **Preprocess text** appropriately for topic modeling (different from sentiment analysis!)\n",
    "- **Interpret topic word lists** and assign meaningful labels\n",
    "- **Think critically** about algorithmic theme detection\n",
    "\n",
    "### üîó Connection to Your Work:\n",
    "This prepares you for **HW4-2**, where you'll apply topic modeling to your own dataset and integrate findings with your HW4-1 term frequency and sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: From Word Counting to Theme Discovery\n",
    "\n",
    "### Your Text Analysis Journey So Far\n",
    "\n",
    "**HW1: Term Frequency**\n",
    "- Question: \"What words appear most often?\"\n",
    "- Output: List of frequent words (\"love\", \"good\", \"time\")\n",
    "- Insight: Surface-level word patterns\n",
    "\n",
    "**HW4-1: Sentiment Analysis**\n",
    "- Question: \"What emotions do these texts express?\"\n",
    "- Output: Positive/negative/neutral scores\n",
    "- Insight: Emotional tone and connotation\n",
    "\n",
    "**Today: Topic Modeling**\n",
    "- Question: \"What hidden themes and subjects are in this collection?\"\n",
    "- Output: Groups of related words that form coherent topics\n",
    "- Insight: Deep thematic structure and subject patterns\n",
    "\n",
    "### ü§î The Critical Question\n",
    "\n",
    "**Can an algorithm truly discover cultural \"themes\"?**\n",
    "\n",
    "Topic modeling doesn't \"understand\" culture‚Äîit finds statistical patterns in word co-occurrence. When you see topics emerge:\n",
    "- The algorithm is clustering words that appear together frequently\n",
    "- **YOU** must interpret whether those clusters represent meaningful cultural themes\n",
    "- This is where humanistic interpretation meets computational analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìñ What is Topic Modeling?\n",
    "\n",
    "**Topic modeling** uses machine learning to discover abstract \"topics\" in a collection of documents.\n",
    "\n",
    "**How it works**:\n",
    "1. Assumes each document contains a mixture of topics\n",
    "2. Assumes each topic is a collection of related words\n",
    "3. Uses statistics to reverse-engineer what those topics might be\n",
    "\n",
    "**Example**: Analyzing 100 movie reviews\n",
    "- **Topic 1**: plot, story, narrative, character, ending (‚Üí *Storytelling*)\n",
    "- **Topic 2**: acting, performance, cast, actor, role (‚Üí *Performance*)\n",
    "- **Topic 3**: visual, effects, cinematography, scene, shot (‚Üí *Visuals*)\n",
    "- **Topic 4**: boring, slow, waste, terrible, disappointing (‚Üí *Negative Critiques*)\n",
    "\n",
    "**Your job as researcher**: Interpret word lists and decide if they form coherent themes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "print(\"üìö Basic libraries imported - ready for topic modeling!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries for topic modeling\n",
    "!pip install gensim\n",
    "!pip install pyLDAvis\n",
    "!pip install nltk\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "print(\"‚úÖ Topic modeling libraries installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Gensim and related libraries\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models import CoherenceModel\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis\n",
    "\n",
    "print(\"‚úÖ Gensim and topic modeling tools ready!\")\n",
    "print(f\"Gensim version: {gensim.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Topic Modeling with Simple Cultural Examples\n",
    "\n",
    "Let's start with a small, clear example to understand how topic modeling works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample cultural data - museum reviews with clear themes\n",
    "museum_reviews = [\n",
    "    # Art-focused reviews\n",
    "    \"The paintings were incredible. Van Gogh's work was beautifully displayed with excellent lighting.\",\n",
    "    \"Amazing art collection! The modern art gallery featured fantastic paintings and sculptures.\",\n",
    "    \"Loved the impressionist paintings. The colors and brushwork were stunning.\",\n",
    "    \n",
    "    # History-focused reviews\n",
    "    \"The ancient artifacts were fascinating. Egyptian mummies and pottery from thousands of years ago.\",\n",
    "    \"Great historical exhibits! Medieval weapons, armor, and manuscripts were well preserved.\",\n",
    "    \"Incredible history museum. Roman coins, Greek pottery, and ancient tools.\",\n",
    "    \n",
    "    # Family/kids reviews\n",
    "    \"Perfect for families! The kids loved the interactive exhibits and hands-on activities.\",\n",
    "    \"Children had a blast. Interactive dinosaur exhibit kept them engaged for hours.\",\n",
    "    \"Great for kids. Educational activities and fun interactive displays throughout.\",\n",
    "    \n",
    "    # Facility/practical reviews\n",
    "    \"The museum cafe was expensive. Gift shop had limited options. Parking was difficult.\",\n",
    "    \"Long lines to enter. Crowded galleries. Overpriced admission tickets and food.\",\n",
    "    \"Clean facilities and friendly staff. Good accessibility for wheelchairs.\"\n",
    "]\n",
    "\n",
    "print(\"üèõÔ∏è Sample Museum Reviews Dataset\")\n",
    "print(f\"Total reviews: {len(museum_reviews)}\")\n",
    "print(\"\\nü§î PREDICTION TIME:\")\n",
    "print(\"What topics do YOU expect topic modeling to discover?\")\n",
    "print(\"Write down 3-4 topic predictions before we run the analysis...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù Your Predictions:\n",
    "\n",
    "**Topic 1**: _____________________\n",
    "\n",
    "**Topic 2**: _____________________\n",
    "\n",
    "**Topic 3**: _____________________\n",
    "\n",
    "**Topic 4**: _____________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Text Preprocessing for Topic Modeling\n",
    "\n",
    "### ‚ö†Ô∏è Different Analysis = Different Preprocessing\n",
    "\n",
    "**For VADER Sentiment Analysis (HW4-1)**:\n",
    "- ‚úÖ Keep punctuation (\"good!!!\" vs \"good\")\n",
    "- ‚úÖ Keep capitalization (\"AMAZING\" vs \"amazing\")\n",
    "- ‚úÖ Keep emojis (üòç, ‚ù§Ô∏è)\n",
    "\n",
    "**For Topic Modeling (Today)**:\n",
    "- ‚ùå Remove punctuation (not meaningful for topics)\n",
    "- ‚ùå Remove capitalization (\"Art\" and \"art\" are same word)\n",
    "- ‚úÖ Lemmatize words (\"running\" ‚Üí \"run\", \"paintings\" ‚Üí \"painting\")\n",
    "- ‚úÖ Remove short words (\"a\", \"an\", \"the\")\n",
    "- ‚úÖ Remove domain-specific stopwords\n",
    "\n",
    "**Why the difference?** \n",
    "- Sentiment = emotional intensity matters (\"good\" vs \"GOOD!!!\")\n",
    "- Topics = semantic meaning matters (\"painting\", \"paintings\", \"painted\" = same concept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced stopwords list for topic modeling\n",
    "stopwords = [\n",
    "    # Basic English stopwords\n",
    "    \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\",\n",
    "    \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\",\n",
    "    \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\",\n",
    "    \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\",\n",
    "    \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\",\n",
    "    \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\",\n",
    "    \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\",\n",
    "    \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\",\n",
    "    \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\",\n",
    "    \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\",\n",
    "    \"how\", \"all\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\",\n",
    "    \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\",\n",
    "    \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\", \"ve\", \"ll\", \"amp\",\n",
    "    \n",
    "    # Additional words that don't help with topics\n",
    "    \"also\", \"would\", \"could\", \"get\", \"go\", \"one\", \"two\", \"see\", \"time\", \"way\",\n",
    "    \"may\", \"said\", \"say\", \"new\", \"first\", \"last\", \"long\", \"little\", \"much\",\n",
    "    \"well\", \"still\", \"even\", \"back\", \"good\", \"many\", \"make\", \"made\", \"us\", \"really\",\n",
    "    \n",
    "    # Museum-specific stopwords (domain-specific)\n",
    "    \"museum\", \"exhibit\", \"exhibition\", \"visit\", \"visited\", \"visitor\", \"review\"\n",
    "]\n",
    "\n",
    "print(f\"‚úÖ Stopwords list loaded: {len(stopwords)} words to filter out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_for_topics(text):\n",
    "    \"\"\"\n",
    "    Aggressive text preprocessing for topic modeling\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation and split into words\n",
    "    words = re.findall(r'\\b[a-z]+\\b', text)\n",
    "    \n",
    "    # Remove stopwords and short words (< 3 characters)\n",
    "    words = [word for word in words if word not in stopwords and len(word) >= 3]\n",
    "    \n",
    "    # Lemmatize words (reduce to base form)\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    return words\n",
    "\n",
    "# Test the preprocessing\n",
    "test_text = \"The paintings were absolutely AMAZING!!! I loved the colorful artworks.\"\n",
    "processed = preprocess_for_topics(test_text)\n",
    "\n",
    "print(\"Text Preprocessing Test:\")\n",
    "print(f\"Original: {test_text}\")\n",
    "print(f\"Processed: {processed}\")\n",
    "print(\"\\nNotice: lowercase, no punctuation, lemmatized, stopwords removed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to all museum reviews\n",
    "processed_reviews = [preprocess_for_topics(review) for review in museum_reviews]\n",
    "\n",
    "print(\"‚úÖ Preprocessing complete!\")\n",
    "print(f\"\\nProcessed {len(processed_reviews)} reviews\")\n",
    "print(\"\\nExample processed reviews:\")\n",
    "for i in range(3):\n",
    "    print(f\"{i+1}. {processed_reviews[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Building the Topic Model with Gensim\n",
    "\n",
    "### Step 1: Create Dictionary and Corpus\n",
    "\n",
    "Gensim needs two special data structures:\n",
    "- **Dictionary**: Maps words to numeric IDs\n",
    "- **Corpus**: Represents documents as bags of word IDs and frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Gensim dictionary\n",
    "dictionary = corpora.Dictionary(processed_reviews)\n",
    "\n",
    "print(\"üìñ Dictionary created!\")\n",
    "print(f\"Total unique words: {len(dictionary)}\")\n",
    "print(\"\\nSample word-to-ID mappings:\")\n",
    "for i, (word_id, word) in enumerate(list(dictionary.items())[:10]):\n",
    "    print(f\"  ID {word_id}: {word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create corpus (bag-of-words representation)\n",
    "corpus = [dictionary.doc2bow(review) for review in processed_reviews]\n",
    "\n",
    "print(\"üì¶ Corpus created!\")\n",
    "print(f\"Total documents: {len(corpus)}\")\n",
    "print(\"\\nExample document representation (word_id, frequency):\")\n",
    "print(f\"First review: {corpus[0]}\")\n",
    "print(\"\\nHuman-readable version:\")\n",
    "for word_id, freq in corpus[0]:\n",
    "    print(f\"  '{dictionary[word_id]}' appears {freq} time(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Train the LDA Model\n",
    "\n",
    "**Key Parameters**:\n",
    "- `num_topics`: How many topics to discover (we'll try 4)\n",
    "- `passes`: How many times to go through the entire corpus (more = better but slower)\n",
    "- `alpha`: Controls document-topic distribution (use 'auto' for now)\n",
    "- `eta`: Controls topic-word distribution (use 'auto' for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LDA model\n",
    "num_topics = 4\n",
    "\n",
    "print(f\"ü§ñ Training LDA model with {num_topics} topics...\")\n",
    "print(\"This may take a moment...\\n\")\n",
    "\n",
    "lda_model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=dictionary,\n",
    "    num_topics=num_topics,\n",
    "    random_state=42,  # For reproducibility\n",
    "    passes=10,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    per_word_topics=True\n",
    ")\n",
    "\n",
    "print(\"‚úÖ LDA model training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Interpreting the Topics\n",
    "\n",
    "### üîç The Critical Task: From Word Lists to Themes\n",
    "\n",
    "The model gives you word lists with probabilities. **YOU** must interpret whether they represent coherent cultural themes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the discovered topics\n",
    "print(\"üéØ DISCOVERED TOPICS\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nEach topic shows the top 10 most important words:\\n\")\n",
    "\n",
    "for idx, topic in lda_model.print_topics(num_words=10):\n",
    "    print(f\"Topic {idx}:\")\n",
    "    print(f\"  {topic}\")\n",
    "    print(f\"  Your interpretation: _____________________\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù Topic Labeling Exercise:\n",
    "\n",
    "Look at the word lists above and assign meaningful labels:\n",
    "\n",
    "**Topic 0**: _____________________ (What theme do these words suggest?)\n",
    "\n",
    "**Topic 1**: _____________________ (What theme do these words suggest?)\n",
    "\n",
    "**Topic 2**: _____________________ (What theme do these words suggest?)\n",
    "\n",
    "**Topic 3**: _____________________ (What theme do these words suggest?)\n",
    "\n",
    "**How did your predictions compare to the actual topics discovered?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cleaner topic visualization\n",
    "def display_topics_clean(model, num_words=8):\n",
    "    \"\"\"\n",
    "    Display topics in a more readable format\n",
    "    \"\"\"\n",
    "    for idx in range(model.num_topics):\n",
    "        # Get top words for this topic\n",
    "        words = model.show_topic(idx, num_words)\n",
    "        \n",
    "        print(f\"Topic {idx}:\")\n",
    "        word_list = [word for word, prob in words]\n",
    "        print(f\"  Words: {', '.join(word_list)}\")\n",
    "        print()\n",
    "\n",
    "print(\"üéØ TOPICS IN READABLE FORMAT\")\n",
    "print(\"=\" * 40)\n",
    "display_topics_clean(lda_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize topic word weights\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx in range(num_topics):\n",
    "    # Get top words and their weights\n",
    "    words_weights = lda_model.show_topic(idx, 8)\n",
    "    words = [word for word, weight in words_weights]\n",
    "    weights = [weight for word, weight in words_weights]\n",
    "    \n",
    "    # Create bar chart\n",
    "    axes[idx].barh(range(len(words)), weights, color='skyblue')\n",
    "    axes[idx].set_yticks(range(len(words)))\n",
    "    axes[idx].set_yticklabels(words)\n",
    "    axes[idx].set_xlabel('Weight')\n",
    "    axes[idx].set_title(f'Topic {idx}')\n",
    "    axes[idx].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Topic visualizations complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Analyzing Document-Topic Assignments\n",
    "\n",
    "Let's see which topics the model assigns to each document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dominant topic for each document\n",
    "def get_dominant_topic(ldamodel, corpus, texts):\n",
    "    \"\"\"\n",
    "    Find the dominant topic for each document\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for i, doc in enumerate(corpus):\n",
    "        # Get topic distribution for this document\n",
    "        topic_dist = ldamodel.get_document_topics(doc)\n",
    "        \n",
    "        # Find dominant topic (highest probability)\n",
    "        dominant_topic = max(topic_dist, key=lambda x: x[1])\n",
    "        topic_num = dominant_topic[0]\n",
    "        topic_prob = dominant_topic[1]\n",
    "        \n",
    "        # Get top words for this topic\n",
    "        topic_words = [word for word, prob in ldamodel.show_topic(topic_num, 5)]\n",
    "        \n",
    "        results.append({\n",
    "            'Document': i,\n",
    "            'Dominant_Topic': topic_num,\n",
    "            'Topic_Probability': round(topic_prob, 3),\n",
    "            'Topic_Keywords': ', '.join(topic_words),\n",
    "            'Original_Text': texts[i][:100] + '...'\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Analyze documents\n",
    "doc_topics = get_dominant_topic(lda_model, corpus, museum_reviews)\n",
    "\n",
    "print(\"üìÑ DOCUMENT-TOPIC ASSIGNMENTS\")\n",
    "print(\"=\" * 70)\n",
    "doc_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show documents grouped by topic\n",
    "for topic_num in range(num_topics):\n",
    "    print(f\"\\nüìå TOPIC {topic_num}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    topic_docs = doc_topics[doc_topics['Dominant_Topic'] == topic_num]\n",
    "    print(f\"Documents in this topic: {len(topic_docs)}\")\n",
    "    print(f\"Keywords: {topic_docs.iloc[0]['Topic_Keywords']}\")\n",
    "    print(\"\\nExample texts:\")\n",
    "    \n",
    "    for idx, row in topic_docs.head(3).iterrows():\n",
    "        print(f\"  - {museum_reviews[row['Document']]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí≠ Critical Analysis Questions:\n",
    "\n",
    "**Do the topic assignments make sense?**\n",
    "- Look at the documents grouped under each topic\n",
    "- Do they share a coherent theme?\n",
    "- Where would you disagree with the model?\n",
    "\n",
    "**What does this reveal about the algorithm's interpretation?**\n",
    "- Is it detecting cultural themes or just word co-occurrence?\n",
    "- What human knowledge is required to make these topics meaningful?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Working with Real Cultural Data\n",
    "\n",
    "Now let's try topic modeling with a larger, more realistic cultural dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create larger sample dataset - book reviews\n",
    "book_reviews_data = {\n",
    "    'book': [\n",
    "        'The Great Gatsby', 'The Great Gatsby', 'The Great Gatsby',\n",
    "        '1984', '1984', '1984',\n",
    "        'Pride and Prejudice', 'Pride and Prejudice', 'Pride and Prejudice',\n",
    "        'The Hobbit', 'The Hobbit', 'The Hobbit',\n",
    "        'To Kill a Mockingbird', 'To Kill a Mockingbird', 'To Kill a Mockingbird',\n",
    "        'Harry Potter', 'Harry Potter', 'Harry Potter'\n",
    "    ],\n",
    "    'review': [\n",
    "        # Gatsby reviews - themes: wealth, American Dream, symbolism\n",
    "        \"Fitzgerald's prose is beautiful. The symbolism of the green light and the eyes of Dr. T.J. Eckleburg represents the American Dream and moral decay of the Jazz Age.\",\n",
    "        \"A tragic love story set in the Roaring Twenties. Gatsby's obsession with Daisy and the themes of wealth and class are timeless.\",\n",
    "        \"The parties, the wealth, the glamour - but underneath it's about the hollowness of the American Dream. Nick's narration provides critical distance.\",\n",
    "        \n",
    "        # 1984 reviews - themes: totalitarianism, surveillance, freedom\n",
    "        \"Orwell's dystopian masterpiece about totalitarian surveillance and thought control. Big Brother and the Thought Police are terrifyingly relevant today.\",\n",
    "        \"The Party's control over language and history is chilling. Newspeak and doublethink show how authoritarian regimes manipulate truth and freedom.\",\n",
    "        \"A warning about government surveillance and the loss of individual freedom. The torture scenes in Room 101 are unforgettable.\",\n",
    "        \n",
    "        # Pride and Prejudice reviews - themes: marriage, class, feminism\n",
    "        \"Austen brilliantly satirizes class and marriage in Regency England. Elizabeth's independence and wit make her a proto-feminist heroine.\",\n",
    "        \"The romance between Elizabeth and Darcy is wonderful, but the novel's real strength is its social commentary on women's limited options.\",\n",
    "        \"More than a love story - it's a sharp critique of marriage as economic necessity and the restrictions on women's lives.\",\n",
    "        \n",
    "        # Hobbit reviews - themes: adventure, fantasy, heroism\n",
    "        \"A perfect fantasy adventure! Bilbo's journey from comfortable hobbit to brave hero is inspiring. Dragons, dwarves, and magic rings!\",\n",
    "        \"Tolkien's world-building is incredible. Middle-earth feels real with its own languages, cultures, and epic quests.\",\n",
    "        \"The adventure through Mirkwood, encounters with trolls and goblins, and Smaug the dragon create an unforgettable fantasy epic.\",\n",
    "        \n",
    "        # Mockingbird reviews - themes: racism, justice, childhood\n",
    "        \"Scout's childhood perspective on racial injustice in the American South is powerful. Atticus defending Tom Robinson shows moral courage.\",\n",
    "        \"Lee confronts racism and inequality in Depression-era Alabama. The trial exposes the deep prejudice and injustice in the legal system.\",\n",
    "        \"A coming-of-age story set against racial violence and injustice. Teaches empathy and standing up for what's right.\",\n",
    "        \n",
    "        # Harry Potter reviews - themes: magic, friendship, good vs evil\n",
    "        \"The magical world of Hogwarts is enchanting! Spells, potions, and magical creatures make this fantasy unforgettable.\",\n",
    "        \"Harry, Ron, and Hermione's friendship is the heart of the series. Their loyalty and courage in fighting Voldemort is inspiring.\",\n",
    "        \"More than magic - it's about choosing between good and evil. The battle against dark wizards and Death Eaters is epic.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "book_df = pd.DataFrame(book_reviews_data)\n",
    "\n",
    "print(\"üìö Book Reviews Dataset Created\")\n",
    "print(f\"Total reviews: {len(book_df)}\")\n",
    "print(f\"Unique books: {book_df['book'].nunique()}\")\n",
    "print(\"\\nü§î Before we analyze - what topics do YOU predict will emerge?\")\n",
    "book_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update stopwords for book reviews\n",
    "book_stopwords = stopwords + ['book', 'novel', 'story', 'read', 'reading', 'author', 'write', 'written']\n",
    "\n",
    "# Preprocess book reviews\n",
    "def preprocess_books(text, custom_stopwords):\n",
    "    text = text.lower()\n",
    "    words = re.findall(r'\\b[a-z]+\\b', text)\n",
    "    words = [word for word in words if word not in custom_stopwords and len(word) >= 3]\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return words\n",
    "\n",
    "processed_book_reviews = [preprocess_books(review, book_stopwords) for review in book_df['review']]\n",
    "\n",
    "print(\"‚úÖ Book reviews preprocessed!\")\n",
    "print(f\"\\nExample processed review:\")\n",
    "print(processed_book_reviews[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary and corpus for book reviews\n",
    "book_dictionary = corpora.Dictionary(processed_book_reviews)\n",
    "book_corpus = [book_dictionary.doc2bow(review) for review in processed_book_reviews]\n",
    "\n",
    "print(f\"üìñ Dictionary: {len(book_dictionary)} unique words\")\n",
    "print(f\"üì¶ Corpus: {len(book_corpus)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LDA model on book reviews\n",
    "num_book_topics = 5\n",
    "\n",
    "print(f\"ü§ñ Training LDA model with {num_book_topics} topics...\\n\")\n",
    "\n",
    "book_lda = LdaModel(\n",
    "    corpus=book_corpus,\n",
    "    id2word=book_dictionary,\n",
    "    num_topics=num_book_topics,\n",
    "    random_state=42,\n",
    "    passes=15,\n",
    "    alpha='auto',\n",
    "    eta='auto'\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model training complete!\\n\")\n",
    "print(\"üéØ DISCOVERED TOPICS IN BOOK REVIEWS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for idx in range(num_book_topics):\n",
    "    words = book_lda.show_topic(idx, 10)\n",
    "    word_list = [word for word, prob in words]\n",
    "    print(f\"\\nTopic {idx}: {', '.join(word_list)}\")\n",
    "    print(f\"Your interpretation: _____________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Experimenting with Number of Topics\n",
    "\n",
    "### üî¨ The Critical Question: How Many Topics?\n",
    "\n",
    "There's no \"correct\" number of topics! It depends on:\n",
    "- Size of your dataset\n",
    "- Diversity of themes\n",
    "- Your research questions\n",
    "- Interpretability of results\n",
    "\n",
    "Let's try different numbers and compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models with different topic numbers\n",
    "def compare_topic_numbers(corpus, dictionary, topic_range=[3, 5, 7]):\n",
    "    \"\"\"\n",
    "    Train models with different numbers of topics and compare\n",
    "    \"\"\"\n",
    "    for num in topic_range:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"MODEL WITH {num} TOPICS\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        model = LdaModel(\n",
    "            corpus=corpus,\n",
    "            id2word=dictionary,\n",
    "            num_topics=num,\n",
    "            random_state=42,\n",
    "            passes=10\n",
    "        )\n",
    "        \n",
    "        for idx in range(num):\n",
    "            words = model.show_topic(idx, 8)\n",
    "            word_list = [word for word, prob in words]\n",
    "            print(f\"Topic {idx}: {', '.join(word_list)}\")\n",
    "\n",
    "compare_topic_numbers(book_corpus, book_dictionary, [3, 5, 7])\n",
    "\n",
    "print(\"\\nüí≠ Which number of topics gives the most interpretable results?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Critical Analysis - When Topic Modeling Fails\n",
    "\n",
    "Let's test topic modeling's limits with challenging examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create challenging dataset - reviews with irony, sarcasm, mixed themes\n",
    "challenging_reviews = [\n",
    "    \"This book was so bad it was actually entertaining. A masterpiece of terrible writing.\",\n",
    "    \"The author clearly tried to write a thriller but accidentally created comedy gold.\",\n",
    "    \"I loved how the romantic subplot completely contradicted the dystopian themes.\",\n",
    "    \"The prose was beautiful but the plot made absolutely no sense whatsoever.\",\n",
    "    \"A perfect example of how not to write a mystery novel. Thank you for this lesson.\"\n",
    "]\n",
    "\n",
    "print(\"ü§î CHALLENGING CASES FOR TOPIC MODELING\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nThese reviews contain:\")\n",
    "print(\"- Irony and sarcasm\")\n",
    "print(\"- Mixed or contradictory themes\")\n",
    "print(\"- Complex human judgment\")\n",
    "print(\"\\nCan topic modeling handle these? Let's find out...\\n\")\n",
    "\n",
    "# Process and analyze\n",
    "processed_challenging = [preprocess_books(r, book_stopwords) for r in challenging_reviews]\n",
    "challenge_dict = corpora.Dictionary(processed_challenging)\n",
    "challenge_corpus = [challenge_dict.doc2bow(r) for r in processed_challenging]\n",
    "\n",
    "challenge_lda = LdaModel(\n",
    "    corpus=challenge_corpus,\n",
    "    id2word=challenge_dict,\n",
    "    num_topics=2,\n",
    "    random_state=42,\n",
    "    passes=10\n",
    ")\n",
    "\n",
    "for idx in range(2):\n",
    "    words = challenge_lda.show_topic(idx, 8)\n",
    "    word_list = [word for word, prob in words]\n",
    "    print(f\"Topic {idx}: {', '.join(word_list)}\")\n",
    "\n",
    "print(\"\\nüí≠ Do these topics capture the irony and complexity?\")\n",
    "print(\"What human knowledge is required to understand these reviews?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Preparing for HW4-2\n",
    "\n",
    "You now have the skills for HW4-2! Let's review the complete workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete workflow summary\n",
    "print(\"üéØ HW4-2 WORKFLOW CHECKLIST\")\n",
    "print(\"=\" * 50)\n",
    "print(\"‚úÖ 1. Review your HW4-1 predictions about topics\")\n",
    "print(\"‚úÖ 2. Preprocess text for topic modeling (aggressive cleaning)\")\n",
    "print(\"‚úÖ 3. Create Gensim dictionary and corpus\")\n",
    "print(\"‚úÖ 4. Train LDA model (experiment with number of topics)\")\n",
    "print(\"‚úÖ 5. Interpret topic word lists and assign labels\")\n",
    "print(\"‚úÖ 6. Analyze document-topic assignments\")\n",
    "print(\"‚úÖ 7. Compare predictions to actual discovered topics\")\n",
    "print(\"‚úÖ 8. Integrate with HW4-1 term frequency and sentiment findings\")\n",
    "print(\"‚úÖ 9. Reflect on complete analytical journey\")\n",
    "\n",
    "print(\"\\nüöÄ You're ready for HW4-2!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Key Takeaways for HW4-2:\n",
    "\n",
    "**Technical**:\n",
    "- Preprocessing for topic modeling is MORE aggressive than for sentiment\n",
    "- Number of topics is a research decision, not a technical one\n",
    "- Topic interpretation requires human judgment and domain knowledge\n",
    "\n",
    "**Critical Thinking**:\n",
    "- Topics are statistical patterns, not guaranteed cultural themes\n",
    "- YOU must validate whether word clusters represent meaningful topics\n",
    "- Some texts (irony, mixed themes) will challenge the algorithm\n",
    "\n",
    "**Research Process**:\n",
    "- Form predictions before running the model\n",
    "- Test different parameter settings\n",
    "- Read individual documents to validate topic assignments\n",
    "- Integrate findings with previous analyses (term frequency, sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: From Words to Hidden Themes\n",
    "\n",
    "Today you learned to:\n",
    "\n",
    "**Technical Skills**:\n",
    "- ‚úÖ Install and use Gensim for topic modeling\n",
    "- ‚úÖ Preprocess text appropriately for LDA\n",
    "- ‚úÖ Create dictionaries and corpus representations\n",
    "- ‚úÖ Train LDA models and experiment with parameters\n",
    "- ‚úÖ Interpret topic word distributions\n",
    "- ‚úÖ Analyze document-topic assignments\n",
    "\n",
    "**Critical Thinking**:\n",
    "- ‚úÖ Distinguish statistical patterns from cultural meanings\n",
    "- ‚úÖ Recognize limitations of algorithmic theme detection\n",
    "- ‚úÖ Question what counts as a \"topic\" or \"theme\"\n",
    "- ‚úÖ Validate computational results with close reading\n",
    "\n",
    "**Research Skills**:\n",
    "- ‚úÖ Form and test hypotheses about hidden themes\n",
    "- ‚úÖ Integrate multiple analytical methods (frequency, sentiment, topics)\n",
    "- ‚úÖ Make interpretive decisions about model parameters\n",
    "- ‚úÖ Document analytical journey from assumptions to insights\n",
    "\n",
    "### üéØ Next Steps:\n",
    "\n",
    "Apply these skills to your HW4-1 dataset in **HW4-2**, where you'll:\n",
    "1. Test your topic predictions from HW4-1\n",
    "2. Discover hidden themes using Gensim LDA\n",
    "3. Integrate findings across all three analytical methods\n",
    "4. Reflect on your complete text analysis journey\n",
    "\n",
    "**Remember**: Being surprised by what topics emerge is a sign of genuine discovery, not analytical failure. The best insights come when data challenges our assumptions!\n",
    "\n",
    "---\n",
    "\n",
    "### üîó Critical Framework Connection: Classification Logic\n",
    "\n",
    "Topic modeling raises fundamental questions about **how code categorizes culture**:\n",
    "- Who decides what counts as a coherent \"topic\"?\n",
    "- What cultural knowledge is embedded in stopword lists and preprocessing choices?\n",
    "- How do algorithmic categories relate to human cultural understanding?\n",
    "- What gets lost when we reduce texts to bags of words?\n",
    "\n",
    "These aren't just technical questions‚Äîthey're about **power, interpretation, and how computational tools shape our understanding of culture**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
