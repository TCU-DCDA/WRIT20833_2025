{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/TCU-DCDA/WRIT20833-2025/blob/main/notebooks/codeAlongs/WRIT20833_Topic_Modeling_Part2_Research_Application_F25.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling Part 2: Research Application\n",
    "## Advanced Techniques and Preparation for HW4-2\n",
    "\n",
    "Welcome to Part 2! Now that you understand LDA basics, we'll work with realistic datasets and prepare for HW4-2.\n",
    "\n",
    "### üéØ What You'll Learn:\n",
    "- **Apply topic modeling** to larger, more realistic cultural datasets\n",
    "- **Experiment with parameters** (especially number of topics)\n",
    "- **Recognize limitations** of topic modeling with challenging texts\n",
    "- **Integrate findings** with HW4-1 (term frequency + sentiment)\n",
    "- **Complete workflow** for HW4-2\n",
    "\n",
    "### üìã Prerequisites:\n",
    "Complete **Part 1: Introduction to Topic Modeling** first!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Import Libraries\n",
    "\n",
    "Let's quickly set up our environment (this should be familiar from Part 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "# Install and import topic modeling libraries\n",
    "!pip install -q gensim nltk\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "\n",
    "print(\"‚úÖ All libraries loaded and ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate our preprocessing function and stopwords from Part 1\n",
    "stopwords = [\n",
    "    \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\",\n",
    "    \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\",\n",
    "    \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\",\n",
    "    \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\",\n",
    "    \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\",\n",
    "    \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\",\n",
    "    \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\",\n",
    "    \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\",\n",
    "    \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\",\n",
    "    \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\",\n",
    "    \"how\", \"all\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\",\n",
    "    \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\",\n",
    "    \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\", \"ve\", \"ll\", \"amp\",\n",
    "    \"also\", \"would\", \"could\", \"get\", \"go\", \"one\", \"two\", \"see\", \"time\", \"way\",\n",
    "    \"may\", \"said\", \"say\", \"new\", \"first\", \"last\", \"long\", \"little\", \"much\",\n",
    "    \"well\", \"still\", \"even\", \"back\", \"good\", \"many\", \"make\", \"made\", \"us\", \"really\"\n",
    "]\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_for_topics(text, custom_stopwords=None):\n",
    "    \"\"\"\n",
    "    Preprocess text for topic modeling with optional custom stopwords\n",
    "    \"\"\"\n",
    "    if custom_stopwords is None:\n",
    "        custom_stopwords = stopwords\n",
    "    \n",
    "    text = text.lower()\n",
    "    words = re.findall(r'\\b[a-z]+\\b', text)\n",
    "    words = [word for word in words if word not in custom_stopwords and len(word) >= 3]\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return words\n",
    "\n",
    "print(\"‚úÖ Preprocessing function ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Working with Realistic Cultural Data\n",
    "\n",
    "Let's work with a larger, more realistic dataset - **book reviews** that span different literary genres and themes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create larger sample dataset - book reviews\n",
    "book_reviews_data = {\n",
    "    'book': [\n",
    "        'The Great Gatsby', 'The Great Gatsby', 'The Great Gatsby',\n",
    "        '1984', '1984', '1984',\n",
    "        'Pride and Prejudice', 'Pride and Prejudice', 'Pride and Prejudice',\n",
    "        'The Hobbit', 'The Hobbit', 'The Hobbit',\n",
    "        'To Kill a Mockingbird', 'To Kill a Mockingbird', 'To Kill a Mockingbird',\n",
    "        'Harry Potter', 'Harry Potter', 'Harry Potter'\n",
    "    ],\n",
    "    'review': [\n",
    "        # Gatsby reviews - themes: wealth, American Dream, symbolism\n",
    "        \"Fitzgerald's prose is beautiful. The symbolism of the green light and the eyes of Dr. T.J. Eckleburg represents the American Dream and moral decay of the Jazz Age.\",\n",
    "        \"A tragic love story set in the Roaring Twenties. Gatsby's obsession with Daisy and the themes of wealth and class are timeless.\",\n",
    "        \"The parties, the wealth, the glamour - but underneath it's about the hollowness of the American Dream. Nick's narration provides critical distance.\",\n",
    "        \n",
    "        # 1984 reviews - themes: totalitarianism, surveillance, freedom\n",
    "        \"Orwell's dystopian masterpiece about totalitarian surveillance and thought control. Big Brother and the Thought Police are terrifyingly relevant today.\",\n",
    "        \"The Party's control over language and history is chilling. Newspeak and doublethink show how authoritarian regimes manipulate truth and freedom.\",\n",
    "        \"A warning about government surveillance and the loss of individual freedom. The torture scenes in Room 101 are unforgettable.\",\n",
    "        \n",
    "        # Pride and Prejudice reviews - themes: marriage, class, feminism\n",
    "        \"Austen brilliantly satirizes class and marriage in Regency England. Elizabeth's independence and wit make her a proto-feminist heroine.\",\n",
    "        \"The romance between Elizabeth and Darcy is wonderful, but the novel's real strength is its social commentary on women's limited options.\",\n",
    "        \"More than a love story - it's a sharp critique of marriage as economic necessity and the restrictions on women's lives.\",\n",
    "        \n",
    "        # Hobbit reviews - themes: adventure, fantasy, heroism\n",
    "        \"A perfect fantasy adventure! Bilbo's journey from comfortable hobbit to brave hero is inspiring. Dragons, dwarves, and magic rings!\",\n",
    "        \"Tolkien's world-building is incredible. Middle-earth feels real with its own languages, cultures, and epic quests.\",\n",
    "        \"The adventure through Mirkwood, encounters with trolls and goblins, and Smaug the dragon create an unforgettable fantasy epic.\",\n",
    "        \n",
    "        # Mockingbird reviews - themes: racism, justice, childhood\n",
    "        \"Scout's childhood perspective on racial injustice in the American South is powerful. Atticus defending Tom Robinson shows moral courage.\",\n",
    "        \"Lee confronts racism and inequality in Depression-era Alabama. The trial exposes the deep prejudice and injustice in the legal system.\",\n",
    "        \"A coming-of-age story set against racial violence and injustice. Teaches empathy and standing up for what's right.\",\n",
    "        \n",
    "        # Harry Potter reviews - themes: magic, friendship, good vs evil\n",
    "        \"The magical world of Hogwarts is enchanting! Spells, potions, and magical creatures make this fantasy unforgettable.\",\n",
    "        \"Harry, Ron, and Hermione's friendship is the heart of the series. Their loyalty and courage in fighting Voldemort is inspiring.\",\n",
    "        \"More than magic - it's about choosing between good and evil. The battle against dark wizards and Death Eaters is epic.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "book_df = pd.DataFrame(book_reviews_data)\n",
    "\n",
    "print(\"üìö Book Reviews Dataset Created\")\n",
    "print(f\"Total reviews: {len(book_df)}\")\n",
    "print(f\"Unique books: {book_df['book'].nunique()}\")\n",
    "print(\"\\nü§î Before we analyze - what topics do YOU predict will emerge?\")\n",
    "book_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù Your Predictions:\n",
    "\n",
    "**Topic 1**: _____________________\n",
    "\n",
    "**Topic 2**: _____________________\n",
    "\n",
    "**Topic 3**: _____________________\n",
    "\n",
    "**Topic 4**: _____________________\n",
    "\n",
    "**Topic 5**: _____________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update stopwords for book reviews (domain-specific)\n",
    "book_stopwords = stopwords + ['book', 'novel', 'story', 'read', 'reading', 'author', 'write', 'written']\n",
    "\n",
    "# Preprocess book reviews\n",
    "processed_book_reviews = [preprocess_for_topics(review, book_stopwords) for review in book_df['review']]\n",
    "\n",
    "print(\"‚úÖ Book reviews preprocessed!\")\n",
    "print(f\"\\nExample processed review:\")\n",
    "print(processed_book_reviews[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary and corpus for book reviews\n",
    "book_dictionary = corpora.Dictionary(processed_book_reviews)\n",
    "book_corpus = [book_dictionary.doc2bow(review) for review in processed_book_reviews]\n",
    "\n",
    "print(f\"üìñ Dictionary: {len(book_dictionary)} unique words\")\n",
    "print(f\"üì¶ Corpus: {len(book_corpus)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LDA model on book reviews\n",
    "num_book_topics = 5\n",
    "\n",
    "print(f\"ü§ñ Training LDA model with {num_book_topics} topics...\\n\")\n",
    "\n",
    "book_lda = LdaModel(\n",
    "    corpus=book_corpus,\n",
    "    id2word=book_dictionary,\n",
    "    num_topics=num_book_topics,\n",
    "    random_state=42,\n",
    "    passes=15,\n",
    "    alpha='auto',\n",
    "    eta='auto'\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model training complete!\\n\")\n",
    "print(\"üéØ DISCOVERED TOPICS IN BOOK REVIEWS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for idx in range(num_book_topics):\n",
    "    words = book_lda.show_topic(idx, 10)\n",
    "    word_list = [word for word, prob in words]\n",
    "    print(f\"\\nTopic {idx}: {', '.join(word_list)}\")\n",
    "    print(f\"Your interpretation: _____________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí≠ Reflection:\n",
    "\n",
    "**How do these topics compare to your predictions?**\n",
    "\n",
    "**Do the word clusters represent coherent literary themes?**\n",
    "\n",
    "**What surprised you?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Experimenting with Number of Topics\n",
    "\n",
    "### üî¨ The Critical Question: How Many Topics?\n",
    "\n",
    "There's no \"correct\" number of topics! It depends on:\n",
    "- Size of your dataset\n",
    "- Diversity of themes\n",
    "- Your research questions\n",
    "- Interpretability of results\n",
    "\n",
    "**This is a research decision, not a technical one.**\n",
    "\n",
    "Let's try different numbers and compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models with different topic numbers\n",
    "def compare_topic_numbers(corpus, dictionary, topic_range=[3, 5, 7]):\n",
    "    \"\"\"\n",
    "    Train models with different numbers of topics and compare\n",
    "    \"\"\"\n",
    "    for num in topic_range:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"MODEL WITH {num} TOPICS\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        model = LdaModel(\n",
    "            corpus=corpus,\n",
    "            id2word=dictionary,\n",
    "            num_topics=num,\n",
    "            random_state=42,\n",
    "            passes=10\n",
    "        )\n",
    "        \n",
    "        for idx in range(num):\n",
    "            words = model.show_topic(idx, 8)\n",
    "            word_list = [word for word, prob in words]\n",
    "            print(f\"Topic {idx}: {', '.join(word_list)}\")\n",
    "\n",
    "compare_topic_numbers(book_corpus, book_dictionary, [3, 5, 7])\n",
    "\n",
    "print(\"\\nüí≠ Which number of topics gives the most interpretable results?\")\n",
    "print(\"Notice how topics get more specific (or fragmented) as the number increases.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Research Decision Guide:\n",
    "\n",
    "**Too Few Topics** (e.g., 3):\n",
    "- Topics are very broad and general\n",
    "- May combine unrelated themes\n",
    "- Good for: High-level overview of large, diverse collections\n",
    "\n",
    "**Too Many Topics** (e.g., 7+):\n",
    "- Topics become fragmented or redundant\n",
    "- May split coherent themes artificially\n",
    "- Good for: Detailed analysis of specialized collections\n",
    "\n",
    "**Just Right** (depends on your data):\n",
    "- Topics are distinct and interpretable\n",
    "- Each captures a coherent theme\n",
    "- **Test multiple values and choose the most meaningful**\n",
    "\n",
    "**For HW4-2**: Start with num_topics = number of categories you predicted, then adjust based on results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Critical Analysis - When Topic Modeling Fails\n",
    "\n",
    "Let's test topic modeling's limits with challenging examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create challenging dataset - reviews with irony, sarcasm, mixed themes\n",
    "challenging_reviews = [\n",
    "    \"This book was so bad it was actually entertaining. A masterpiece of terrible writing.\",\n",
    "    \"The author clearly tried to write a thriller but accidentally created comedy gold.\",\n",
    "    \"I loved how the romantic subplot completely contradicted the dystopian themes.\",\n",
    "    \"The prose was beautiful but the plot made absolutely no sense whatsoever.\",\n",
    "    \"A perfect example of how not to write a mystery novel. Thank you for this lesson.\"\n",
    "]\n",
    "\n",
    "print(\"ü§î CHALLENGING CASES FOR TOPIC MODELING\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nThese reviews contain:\")\n",
    "print(\"- Irony and sarcasm\")\n",
    "print(\"- Mixed or contradictory themes\")\n",
    "print(\"- Complex human judgment\")\n",
    "print(\"\\nCan topic modeling handle these? Let's find out...\\n\")\n",
    "\n",
    "# Process and analyze\n",
    "processed_challenging = [preprocess_for_topics(r, book_stopwords) for r in challenging_reviews]\n",
    "challenge_dict = corpora.Dictionary(processed_challenging)\n",
    "challenge_corpus = [challenge_dict.doc2bow(r) for r in processed_challenging]\n",
    "\n",
    "challenge_lda = LdaModel(\n",
    "    corpus=challenge_corpus,\n",
    "    id2word=challenge_dict,\n",
    "    num_topics=2,\n",
    "    random_state=42,\n",
    "    passes=10\n",
    ")\n",
    "\n",
    "for idx in range(2):\n",
    "    words = challenge_lda.show_topic(idx, 8)\n",
    "    word_list = [word for word, prob in words]\n",
    "    print(f\"Topic {idx}: {', '.join(word_list)}\")\n",
    "\n",
    "print(\"\\nüí≠ Do these topics capture the irony and complexity?\")\n",
    "print(\"What human knowledge is required to understand these reviews?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ö†Ô∏è Topic Modeling Limitations:\n",
    "\n",
    "Topic modeling struggles with:\n",
    "- **Irony and sarcasm** (\"so bad it's good\")\n",
    "- **Context-dependent meaning** (\"sick\" = cool or ill?)\n",
    "- **Coded language** (cultural references, in-group terminology)\n",
    "- **Mixed emotions** (\"beautiful but nonsensical\")\n",
    "- **Small datasets** (<50 documents = unreliable)\n",
    "\n",
    "**Always validate** topic model results with:\n",
    "1. Close reading of individual documents\n",
    "2. Domain expertise and cultural knowledge\n",
    "3. Comparison to other methods (term frequency, sentiment)\n",
    "\n",
    "**For HW4-2**: Discuss where LDA worked well and where it failed with your specific dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Integrating with HW4-1 Analysis\n",
    "\n",
    "### Bringing It All Together\n",
    "\n",
    "HW4-2 asks you to integrate **three analytical methods**:\n",
    "\n",
    "1. **Term Frequency** (HW4-1): What words appear most often?\n",
    "2. **Sentiment Analysis** (HW4-1): What emotions do texts express?\n",
    "3. **Topic Modeling** (Today): What hidden themes emerge?\n",
    "\n",
    "Let's practice this integration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example integration: Analyze one book's reviews across all three methods\n",
    "gatsby_reviews = book_df[book_df['book'] == 'The Great Gatsby']['review'].tolist()\n",
    "\n",
    "print(\"üìä INTEGRATED ANALYSIS: The Great Gatsby Reviews\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Term Frequency\n",
    "all_words = []\n",
    "for review in gatsby_reviews:\n",
    "    all_words.extend(preprocess_for_topics(review, book_stopwords))\n",
    "\n",
    "from collections import Counter\n",
    "word_freq = Counter(all_words).most_common(10)\n",
    "\n",
    "print(\"\\n1Ô∏è‚É£ TERM FREQUENCY (Top 10 Words):\")\n",
    "for word, count in word_freq:\n",
    "    print(f\"   {word}: {count}\")\n",
    "\n",
    "# 2. Sentiment Analysis (would use VADER in actual HW4-2)\n",
    "print(\"\\n2Ô∏è‚É£ SENTIMENT ANALYSIS:\")\n",
    "print(\"   [In HW4-2, you'll show VADER compound scores here]\")\n",
    "\n",
    "# 3. Topic Modeling\n",
    "print(\"\\n3Ô∏è‚É£ TOPIC MODELING:\")\n",
    "print(\"   Based on our 5-topic model, Gatsby reviews cluster around:\")\n",
    "gatsby_indices = book_df[book_df['book'] == 'The Great Gatsby'].index\n",
    "for idx in gatsby_indices:\n",
    "    doc_topics = book_lda.get_document_topics(book_corpus[idx])\n",
    "    dominant = max(doc_topics, key=lambda x: x[1])\n",
    "    print(f\"   Review {idx}: Topic {dominant[0]} ({dominant[1]:.2f} probability)\")\n",
    "\n",
    "print(\"\\nüí° INTEGRATION INSIGHT:\")\n",
    "print(\"Notice how each method reveals different aspects:\")\n",
    "print(\"- Term frequency shows WHAT is discussed\")\n",
    "print(\"- Sentiment shows HOW people feel\")\n",
    "print(\"- Topics show THEMES that connect texts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Complete HW4-2 Workflow\n",
    "\n",
    "You now have all the skills for HW4-2! Here's your complete workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete workflow summary\n",
    "print(\"üéØ HW4-2 COMPLETE WORKFLOW\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nüìã PART 1: REVIEW HW4-1\")\n",
    "print(\"‚úÖ 1. Review your HW4-1 term frequency findings\")\n",
    "print(\"‚úÖ 2. Review your HW4-1 sentiment analysis results\")\n",
    "print(\"‚úÖ 3. Review your predictions about topics\")\n",
    "\n",
    "print(\"\\nüìã PART 2: TOPIC MODELING\")\n",
    "print(\"‚úÖ 4. Preprocess text for topic modeling (aggressive cleaning)\")\n",
    "print(\"‚úÖ 5. Create custom stopwords for your domain\")\n",
    "print(\"‚úÖ 6. Create Gensim dictionary and corpus\")\n",
    "print(\"‚úÖ 7. Experiment with different num_topics (try 3-7)\")\n",
    "print(\"‚úÖ 8. Choose best num_topics based on interpretability\")\n",
    "print(\"‚úÖ 9. Interpret topic word lists and assign labels\")\n",
    "print(\"‚úÖ 10. Analyze document-topic assignments\")\n",
    "\n",
    "print(\"\\nüìã PART 3: INTEGRATION & REFLECTION\")\n",
    "print(\"‚úÖ 11. Compare predictions to actual discovered topics\")\n",
    "print(\"‚úÖ 12. Integrate findings across all three methods\")\n",
    "print(\"‚úÖ 13. Identify where LDA worked well\")\n",
    "print(\"‚úÖ 14. Identify where LDA struggled\")\n",
    "print(\"‚úÖ 15. Reflect on complete analytical journey\")\n",
    "print(\"‚úÖ 16. Connect to Classification Logic framework\")\n",
    "\n",
    "print(\"\\nüöÄ You're ready for HW4-2!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Key Tips for HW4-2:\n",
    "\n",
    "**Technical Tips**:\n",
    "- Add domain-specific stopwords for your dataset\n",
    "- Try num_topics between 3-7 and choose the most interpretable\n",
    "- Increase passes to 15-20 for better results\n",
    "- Save your best model so you don't have to retrain\n",
    "\n",
    "**Critical Thinking Tips**:\n",
    "- Topics are statistical patterns, not guaranteed cultural themes\n",
    "- YOU must validate whether word clusters are meaningful\n",
    "- Read individual documents to verify topic assignments\n",
    "- Some texts (irony, mixed themes) will challenge the algorithm\n",
    "\n",
    "**Research Process Tips**:\n",
    "- Form predictions before running the model\n",
    "- Document surprises and unexpected findings\n",
    "- Compare across all three analytical methods\n",
    "- Discuss limitations honestly in your reflection\n",
    "\n",
    "**Remember**: Being surprised by what topics emerge is a sign of genuine discovery, not analytical failure. The best insights come when data challenges our assumptions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Advanced Topic Modeling\n",
    "\n",
    "Today you learned:\n",
    "\n",
    "**Advanced Skills**:\n",
    "- ‚úÖ Work with realistic, larger cultural datasets\n",
    "- ‚úÖ Experiment with different numbers of topics\n",
    "- ‚úÖ Make research decisions about model parameters\n",
    "- ‚úÖ Recognize when topic modeling fails\n",
    "- ‚úÖ Integrate topic modeling with other analytical methods\n",
    "\n",
    "**Critical Thinking**:\n",
    "- ‚úÖ Understand topic modeling's limitations (irony, sarcasm, context)\n",
    "- ‚úÖ Validate computational results with close reading\n",
    "- ‚úÖ Recognize that interpretability matters more than technical metrics\n",
    "- ‚úÖ Question algorithmic categories and their relationship to culture\n",
    "\n",
    "**Research Skills**:\n",
    "- ‚úÖ Make principled decisions about model parameters\n",
    "- ‚úÖ Integrate findings across multiple analytical approaches\n",
    "- ‚úÖ Document analytical journey from assumptions to insights\n",
    "- ‚úÖ Reflect honestly on methods' strengths and limitations\n",
    "\n",
    "### üéØ Apply to HW4-2:\n",
    "\n",
    "You now have everything you need to:\n",
    "1. Test your topic predictions from HW4-1\n",
    "2. Discover hidden themes using Gensim LDA\n",
    "3. Integrate findings across term frequency, sentiment, and topics\n",
    "4. Reflect on your complete text analysis journey\n",
    "\n",
    "---\n",
    "\n",
    "### üîó Critical Framework Connection: Classification Logic\n",
    "\n",
    "Throughout this analysis, you've engaged with fundamental questions about **how code categorizes culture**:\n",
    "\n",
    "- **Who decides what counts as a coherent \"topic\"?** The algorithm clusters words statistically, but YOU decide if those clusters represent meaningful cultural themes.\n",
    "\n",
    "- **What cultural knowledge is embedded in our choices?** Stopword lists, preprocessing decisions, and topic labels all require human judgment shaped by our cultural positions.\n",
    "\n",
    "- **How do algorithmic categories relate to human understanding?** LDA finds word co-occurrence patterns. Whether those patterns map onto actual cultural themes requires humanistic interpretation.\n",
    "\n",
    "- **What gets lost in computational reading?** Irony, coded language, contextual nuance‚Äîthe algorithm's \"bag of words\" approach misses what makes cultural texts meaningful.\n",
    "\n",
    "These aren't just technical questions‚Äîthey're about **power, interpretation, and how computational tools shape our understanding of culture**. Topic modeling is powerful for pattern detection, but it's your humanistic expertise that transforms those patterns into cultural insight."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
