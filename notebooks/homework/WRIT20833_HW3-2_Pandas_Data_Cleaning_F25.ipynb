{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36141631",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/TCU-DCDA/WRIT20833-2025/blob/main/notebooks/homework/WRIT20833_HW3-2_Pandas_Data_Cleaning_F25.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607d4c26",
   "metadata": {},
   "source": [
    "# Data Cleaning & Analysis: Student Practice Notebook\n",
    "## Apply Your Skills to Real Cultural Data\n",
    "\n",
    "**Name:** ________________________________  \n",
    "**Date:** ________________________________\n",
    "\n",
    "Welcome to your hands-on practice with data cleaning! This notebook is your space to apply the data cleaning and analysis techniques from **Pandas_02** to your own cultural dataset. You'll practice the complete workflow from ethical data collection to meaningful cultural insights.\n",
    "\n",
    "### üéØ Learning Goals:\n",
    "By completing this practice, you will:\n",
    "- **Apply ethical data collection principles** including robots.txt compliance and AI-era considerations\n",
    "- **Systematically identify and document data quality issues** in real cultural datasets\n",
    "- **Use pandas string methods and data handling techniques** to clean messy cultural data\n",
    "- **Create analysis-ready datasets** through systematic cleaning workflows\n",
    "- **Generate cultural insights** from properly cleaned data\n",
    "- **Reflect critically** on how data cleaning choices affect cultural analysis\n",
    "\n",
    "### üîÑ **Continuing Your Cultural Data Journey from HW3-1**\n",
    "**Default Approach: Use Your HW3-1 Dataset**\n",
    "The best learning experience comes from deepening your analysis of the cultural dataset you explored in HW3-1. This continuity allows you to:\n",
    "- **Build expertise** in your chosen cultural domain\n",
    "- **Apply progressive skills** from basic exploration to advanced cleaning\n",
    "- **Reference discoveries** from HW3-1 as you clean and refine the data\n",
    "- **Create comprehensive analysis** that spans multiple skill levels\n",
    "\n",
    "### üìã Dataset Requirements for Effective Cleaning Practice\n",
    "**‚úÖ Your dataset should have opportunities for:**\n",
    "- [ ] **Text standardization** (inconsistent capitalization, formatting variations)\n",
    "- [ ] **Missing data handling** (empty cells, null values, incomplete records)\n",
    "- [ ] **Category consolidation** (similar values with different names/spellings)\n",
    "- [ ] **Data type issues** (numbers stored as text, date formatting problems)\n",
    "- [ ] **Duplicate detection** (exact or near-duplicate entries)\n",
    "\n",
    "### üîÄ **When to Consider Switching Datasets**\n",
    "**You may choose a different dataset from HW3-1 if:**\n",
    "- [ ] **Too clean**: Your HW3-1 data has no missing values, perfect formatting, and no inconsistencies\n",
    "- [ ] **Too small**: Fewer than 15 rows limits meaningful cleaning practice\n",
    "- [ ] **Wrong focus**: Your research interests have shifted to a different cultural domain\n",
    "- [ ] **Limited complexity**: Only text OR only numbers (missing mixed data types)\n",
    "\n",
    "### üìÇ **If Switching: Recommended Dataset Sources**\n",
    "- **Kaggle**: Movies, books, music, museums, historical records\n",
    "- **Government data**: Cultural statistics, census data, arts funding  \n",
    "- **Academic repositories**: Digital humanities collections\n",
    "- **Cultural institutions**: Museum collections, library catalogs\n",
    "\n",
    "**‚ö†Ô∏è Important**: If you switch datasets, ensure it meets the data complexity requirements for meaningful cleaning practice!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab05909e",
   "metadata": {},
   "source": [
    "## Part 0: Understanding Data Collection Ethics & Cultural Responsibility\n",
    "\n",
    "Before diving into data cleaning, let's establish the ethical foundation for responsible cultural data analysis in the contemporary digital landscape.\n",
    "\n",
    "### üåê Found Data Ethics & robots.txt\n",
    "When working with cultural datasets, especially those scraped from websites, it's crucial to understand **robots.txt** - the web's ethical guidelines for automated data collection.\n",
    "\n",
    "**üìù How to Check robots.txt Compliance:**\n",
    "- Add `/robots.txt` to any website URL (e.g., `https://example.com/robots.txt`)\n",
    "- This file specifies what automated data collection is permitted\n",
    "- **Always respect these guidelines** in your research\n",
    "\n",
    "**üí° Example robots.txt concerns for cultural sites:**\n",
    "- Museum websites often restrict scraping of collection databases\n",
    "- Literary archives may limit automated access to protect copyright\n",
    "- Social media platforms restrict bulk downloading of cultural content\n",
    "\n",
    "### ü§ñ AI Era Considerations: LLMs and Data Ethics\n",
    "The rise of AI training has created new ethical debates around data collection and use:\n",
    "\n",
    "**Key Contemporary Issues:**\n",
    "- **Training Data**: Many websites now explicitly restrict their content from AI training\n",
    "- **Attribution**: Cultural data creators expect proper credit and context\n",
    "- **Scale**: Large-scale scraping can burden cultural institutions' servers\n",
    "- **Purpose**: Research goals should respect the intentions of data creators\n",
    "\n",
    "**üìã Ethical Checklist for Cultural Data:**\n",
    "- [ ] **Source attribution**: Credit your data sources appropriately\n",
    "- [ ] **Robots.txt compliance**: Verify data collection followed website guidelines\n",
    "- [ ] **Scale appropriateness**: Use only the data you need for your research questions\n",
    "- [ ] **Purpose transparency**: Be clear about your research goals\n",
    "- [ ] **Cultural sensitivity**: Consider the communities and cultures represented in your data\n",
    "\n",
    "### üé≠ Cultural Data & Representation\n",
    "Cultural datasets carry special responsibilities:\n",
    "- **Whose voices are included?** Consider who created the original cultural artifacts\n",
    "- **What biases exist?** Historical data often reflects the perspectives of dominant groups\n",
    "- **How might cleaning affect meaning?** Standardization can erase important cultural nuances\n",
    "- **What's missing?** Absence of data is often as significant as what's present"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841afc72",
   "metadata": {},
   "source": [
    "### ü§î Reflection: Your Dataset's Ethics & Cultural Context\n",
    "**Answer these questions about your chosen dataset before beginning analysis:**\n",
    "\n",
    "1. **Are you continuing with your HW3-1 dataset or switching? Why?**\n",
    "\n",
    "2. **What cultural domain does your dataset represent? (literature, art, music, film, etc.)**\n",
    "\n",
    "3. **Where did this data originate? Who collected it and for what purpose?**\n",
    "\n",
    "4. **If web-scraped, were ethical guidelines (robots.txt) followed during collection?**\n",
    "\n",
    "5. **What biases might exist in how this cultural data was categorized or collected?**\n",
    "\n",
    "6. **Whose voices or perspectives might be missing from this dataset?**\n",
    "\n",
    "7. **How should you properly attribute this data source in your research?**\n",
    "\n",
    "8. **What cultural sensitivities should you consider during analysis?**\n",
    "\n",
    "9. **How might your data cleaning choices affect the cultural meaning of the information?**\n",
    "\n",
    "10. **If continuing from HW3-1: What data quality issues did you notice during your initial exploration?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06d071f",
   "metadata": {},
   "source": [
    "## Part 1: Loading and Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0af301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Set display options for better readability\n",
    "pd.options.display.max_rows = 100\n",
    "pd.options.display.max_columns = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2e3c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your cultural dataset\n",
    "# Replace 'your_filename.csv' with your actual file name\n",
    "# For other file types, use: pd.read_excel(), pd.read_json(), etc.\n",
    "\n",
    "my_data = pd.read_csv('your_filename.csv')\n",
    "\n",
    "print(f\"‚úÖ Loaded dataset with {len(my_data)} rows and {len(my_data.columns)} columns\")\n",
    "print(f\"Dataset shape: {my_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df1e8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First look at your data\n",
    "print(\"First 5 rows:\")\n",
    "my_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd7a5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get basic information about your dataset\n",
    "print(\"Dataset info:\")\n",
    "my_data.info()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "print(\"Column names:\")\n",
    "print(list(my_data.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bea1709",
   "metadata": {},
   "source": [
    "### ü§î Reflection: Initial Observations\n",
    "**Write your observations about your dataset:**\n",
    "1. **What cultural topic does this data represent?**\n",
    "\n",
    "2. **What are the main columns and what do they represent?**\n",
    "\n",
    "3. **What data types do you see? (text, numbers, dates, etc.)**\n",
    "\n",
    "4. **What potential data quality issues do you notice?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52626c9",
   "metadata": {},
   "source": [
    "## Part 2: Identifying Data Problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0381c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing data\n",
    "print(\"Missing data summary:\")\n",
    "missing_data = my_data.isnull().sum()\n",
    "print(missing_data[missing_data > 0])  # Only show columns with missing data\n",
    "\n",
    "print(f\"\\nTotal missing values: {my_data.isnull().sum().sum()}\")\n",
    "print(f\"Percentage of missing data: {(my_data.isnull().sum().sum() / (len(my_data) * len(my_data.columns))) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27192c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine unique values in text/categorical columns\n",
    "print(\"Unique values in categorical columns:\")\n",
    "print(\"(Look for inconsistencies, duplicates, and formatting issues)\\n\")\n",
    "\n",
    "for col in my_data.select_dtypes(include=['object']).columns:\n",
    "    unique_count = my_data[col].nunique()\n",
    "    print(f\"{col} ({unique_count} unique values):\")\n",
    "    if unique_count <= 20:  # Show all if 20 or fewer\n",
    "        print(f\"  {sorted(my_data[col].dropna().unique())}\")\n",
    "    else:  # Show first 10 if more than 20\n",
    "        print(f\"  First 10: {sorted(my_data[col].dropna().unique())[:10]}\")\n",
    "        print(f\"  ... and {unique_count - 10} more\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd478e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for potential duplicates\n",
    "duplicate_rows = my_data.duplicated().sum()\n",
    "print(f\"Complete duplicate rows: {duplicate_rows}\")\n",
    "\n",
    "if duplicate_rows > 0:\n",
    "    print(\"\\nDuplicate rows:\")\n",
    "    print(my_data[my_data.duplicated()])\n",
    "else:\n",
    "    print(\"‚úÖ No complete duplicate rows found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7457fd",
   "metadata": {},
   "source": [
    "### üìù Data Problems Identified:\n",
    "**List the specific issues you found in your data:**\n",
    "\n",
    "**Missing Data:**\n",
    "- \n",
    "\n",
    "**Text/Formatting Issues:**\n",
    "- \n",
    "\n",
    "**Duplicate Issues:**\n",
    "- \n",
    "\n",
    "**Other Problems:**\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f555ba2",
   "metadata": {},
   "source": [
    "## Part 3: Creating Your Data Cleaning Plan\n",
    "\n",
    "Before cleaning, create a systematic plan based on your data problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17ed982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for cleaning (preserve original)\n",
    "my_data_cleaned = my_data.copy()\n",
    "\n",
    "print(\"‚úÖ Created copy for cleaning\")\n",
    "print(f\"Original data: {len(my_data)} rows\")\n",
    "print(f\"Working copy: {len(my_data_cleaned)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e9fcbd",
   "metadata": {},
   "source": [
    "### üéØ My Cleaning Strategy:\n",
    "**Write your step-by-step plan:**\n",
    "\n",
    "1. **Missing Data Strategy:**\n",
    "   - \n",
    "\n",
    "2. **Text Standardization Tasks:**\n",
    "   - \n",
    "\n",
    "3. **Category Cleaning:**\n",
    "   - \n",
    "\n",
    "4. **Duplicate Handling:**\n",
    "   - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24d2426",
   "metadata": {},
   "source": [
    "## Part 4: Text Cleaning and Standardization\n",
    "\n",
    "Apply pandas string methods to clean and standardize your text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b192b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Standardize capitalization for a text column\n",
    "# Replace 'column_name' with your actual column name\n",
    "\n",
    "# Before cleaning:\n",
    "# print(\"Before standardization:\")\n",
    "# print(my_data_cleaned['column_name'].unique())\n",
    "\n",
    "# Apply title case:\n",
    "# my_data_cleaned['column_name'] = my_data_cleaned['column_name'].str.title()\n",
    "\n",
    "# After cleaning:\n",
    "# print(\"\\nAfter standardization:\")\n",
    "# print(my_data_cleaned['column_name'].unique())\n",
    "\n",
    "print(\"Add your text cleaning code here\")\n",
    "print(\"Use the examples from the main lesson as templates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528402ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Create category mappings to standardize similar values\n",
    "# Replace with your actual column and categories\n",
    "\n",
    "# category_mapping = {\n",
    "#     'old_value_1': 'Standard_Value_1',\n",
    "#     'old_value_2': 'Standard_Value_1',  # Multiple old values can map to same new value\n",
    "#     'old_value_3': 'Standard_Value_2'\n",
    "# }\n",
    "\n",
    "# Apply mapping:\n",
    "# my_data_cleaned['column_name_clean'] = my_data_cleaned['column_name'].str.lower()\n",
    "# my_data_cleaned['column_name_clean'] = my_data_cleaned['column_name_clean'].replace(category_mapping)\n",
    "# my_data_cleaned['column_name_clean'] = my_data_cleaned['column_name_clean'].str.title()\n",
    "\n",
    "print(\"Add your category mapping code here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7bf665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing data for text columns\n",
    "# Choose appropriate strategy based on your data\n",
    "\n",
    "# Option 1: Fill with placeholder\n",
    "# my_data_cleaned['column_name'] = my_data_cleaned['column_name'].fillna('Unknown')\n",
    "\n",
    "# Option 2: Drop rows with missing values in critical columns\n",
    "# my_data_cleaned = my_data_cleaned.dropna(subset=['critical_column'])\n",
    "\n",
    "print(\"Add your missing data handling code here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe08e60f",
   "metadata": {},
   "source": [
    "## Part 5: Handling Numeric Data and Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e69d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing numeric data\n",
    "# Choose strategy based on your data and research questions\n",
    "\n",
    "# Option 1: Fill with median (robust to outliers)\n",
    "# numeric_column = 'your_numeric_column'\n",
    "# median_value = my_data_cleaned[numeric_column].median()\n",
    "# my_data_cleaned[numeric_column] = my_data_cleaned[numeric_column].fillna(median_value)\n",
    "# print(f\"Filled missing {numeric_column} with median: {median_value}\")\n",
    "\n",
    "# Option 2: Fill with mean\n",
    "# mean_value = my_data_cleaned[numeric_column].mean()\n",
    "# my_data_cleaned[numeric_column] = my_data_cleaned[numeric_column].fillna(mean_value)\n",
    "\n",
    "# Option 3: Fill based on category groups\n",
    "# my_data_cleaned[numeric_column] = my_data_cleaned.groupby('category_column')[numeric_column].transform(lambda x: x.fillna(x.median()))\n",
    "\n",
    "print(\"Add your numeric data cleaning code here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d646ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data types if needed\n",
    "# Example: Convert string numbers to numeric\n",
    "# my_data_cleaned['numeric_column'] = pd.to_numeric(my_data_cleaned['numeric_column'], errors='coerce')\n",
    "\n",
    "# Example: Convert string dates to datetime\n",
    "# my_data_cleaned['date_column'] = pd.to_datetime(my_data_cleaned['date_column'], errors='coerce')\n",
    "\n",
    "print(\"Add data type conversion code here if needed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7538d803",
   "metadata": {},
   "source": [
    "## Part 6: Checking Your Cleaning Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be10cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare before and after cleaning\n",
    "print(\"CLEANING SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Original dataset: {len(my_data)} rows, {len(my_data.columns)} columns\")\n",
    "print(f\"Cleaned dataset: {len(my_data_cleaned)} rows, {len(my_data_cleaned.columns)} columns\")\n",
    "print(f\"Rows removed: {len(my_data) - len(my_data_cleaned)}\")\n",
    "print(f\"Columns added: {len(my_data_cleaned.columns) - len(my_data.columns)}\")\n",
    "\n",
    "print(\"\\nMissing data comparison:\")\n",
    "print(f\"Original missing values: {my_data.isnull().sum().sum()}\")\n",
    "print(f\"Cleaned missing values: {my_data_cleaned.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c06404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final check of cleaned data\n",
    "print(\"Cleaned data sample:\")\n",
    "my_data_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccf5849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check remaining data issues\n",
    "print(\"Remaining missing data:\")\n",
    "remaining_missing = my_data_cleaned.isnull().sum()\n",
    "print(remaining_missing[remaining_missing > 0])\n",
    "\n",
    "print(\"\\nData types:\")\n",
    "print(my_data_cleaned.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8818562",
   "metadata": {},
   "source": [
    "## Part 7: Data Analysis and Exploration\n",
    "\n",
    "Now that your data is clean, perform some exploratory analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51838726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic descriptive statistics for numeric columns\n",
    "print(\"Descriptive statistics:\")\n",
    "my_data_cleaned.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d24701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value counts for categorical columns\n",
    "# Replace 'category_column' with your actual column name\n",
    "\n",
    "# print(\"Distribution of categories:\")\n",
    "# category_counts = my_data_cleaned['category_column'].value_counts()\n",
    "# print(category_counts)\n",
    "\n",
    "print(\"Add your categorical analysis code here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca45ec3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groupby analysis (if you have both categorical and numeric data)\n",
    "# Replace column names with your actual columns\n",
    "\n",
    "# grouped_analysis = my_data_cleaned.groupby('category_column').agg({\n",
    "#     'numeric_column_1': ['mean', 'count'],\n",
    "#     'numeric_column_2': ['sum', 'median']\n",
    "# })\n",
    "# print(\"Analysis by category:\")\n",
    "# print(grouped_analysis)\n",
    "\n",
    "print(\"Add your groupby analysis code here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe5a998",
   "metadata": {},
   "source": [
    "## Part 8: Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b209646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations appropriate for your data\n",
    "# Examples:\n",
    "\n",
    "# Bar chart for categorical data:\n",
    "# category_counts = my_data_cleaned['category_column'].value_counts()\n",
    "# category_counts.plot(kind='bar', title='Distribution of Categories', figsize=(10, 6))\n",
    "# plt.xlabel('Category')\n",
    "# plt.ylabel('Count')\n",
    "# plt.xticks(rotation=45)\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "print(\"Add your visualization code here\")\n",
    "print(\"Consider: bar charts, histograms, scatter plots, or time series plots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1795c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second visualization\n",
    "# Example: Histogram for numeric data:\n",
    "# my_data_cleaned['numeric_column'].hist(bins=20, title='Distribution of Numeric Values', figsize=(8, 6))\n",
    "# plt.xlabel('Value')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.show()\n",
    "\n",
    "print(\"Add a second visualization here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ab77fc",
   "metadata": {},
   "source": [
    "## Part 9: Save Your Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6e0aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save your cleaned dataset\n",
    "output_filename = 'my_cleaned_cultural_data.csv'\n",
    "my_data_cleaned.to_csv(output_filename, index=False)\n",
    "print(f\"‚úÖ Saved cleaned dataset as: {output_filename}\")\n",
    "\n",
    "# Optional: Save analysis results\n",
    "# if 'grouped_analysis' in locals():\n",
    "#     grouped_analysis.to_csv('my_analysis_results.csv')\n",
    "#     print(\"‚úÖ Saved analysis results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dce7a3b",
   "metadata": {},
   "source": [
    "## Part 10: Reflection and Cultural Insights\n",
    "\n",
    "### üéØ Data Cleaning Reflection (Connect to Pandas_02 Lesson):\n",
    "**How did the techniques from the Pandas_02 lesson work with your specific dataset?**\n",
    "\n",
    "\n",
    "**What challenges did you encounter that weren't covered in the lesson examples?**\n",
    "\n",
    "\n",
    "**Which pandas string methods (.str.title(), .str.replace(), etc.) were most useful for your data?**\n",
    "\n",
    "\n",
    "**How did your missing data strategy compare to the approaches demonstrated in the lesson?**\n",
    "\n",
    "\n",
    "### üîÑ Dataset Continuity Experience:\n",
    "**If you continued with your HW3-1 dataset: How did working with the same data across both assignments enhance your learning?**\n",
    "\n",
    "\n",
    "**What new insights about your cultural domain emerged through the cleaning process?**\n",
    "\n",
    "\n",
    "**How did your understanding of the data's limitations and biases evolve from HW3-1 to HW3-2?**\n",
    "\n",
    "\n",
    "**If you switched datasets: What did you learn from comparing the data quality issues in both datasets?**\n",
    "\n",
    "\n",
    "### üìä Cultural Analysis Insights:\n",
    "**What patterns or trends did you discover in your cleaned data?**\n",
    "\n",
    "\n",
    "**How do your findings compare to the literary dataset patterns shown in Pandas_02?**\n",
    "\n",
    "\n",
    "**What cultural questions does your analysis raise that require further investigation?**\n",
    "\n",
    "\n",
    "**How might data quality issues have affected historical or cultural research using similar datasets?**\n",
    "\n",
    "\n",
    "### ü§ñ Ethics and AI-Era Considerations:\n",
    "**How did considering robots.txt and data collection ethics affect your analysis approach?**\n",
    "\n",
    "\n",
    "**What biases or limitations did you discover in your dataset during the cleaning process?**\n",
    "\n",
    "\n",
    "**How might AI training considerations influence how cultural institutions share their data in the future?**\n",
    "\n",
    "\n",
    "**What responsibilities do cultural data analysts have in the current AI landscape?**\n",
    "\n",
    "\n",
    "### üîç Next Steps and Future Research:\n",
    "**What additional data cleaning techniques would improve your analysis?**\n",
    "\n",
    "\n",
    "**What other cultural datasets could you combine with this one for richer insights?**\n",
    "\n",
    "\n",
    "**How could this cleaned data contribute to broader digital humanities or cultural research projects?**\n",
    "\n",
    "\n",
    "**What questions would you ask the original data collectors about their methodology?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91a3dae",
   "metadata": {},
   "source": [
    "## üìö Summary: Your Data Cleaning Journey\n",
    "\n",
    "Congratulations! You've successfully applied the data cleaning techniques from **Pandas_02** to your own cultural dataset!\n",
    "\n",
    "### ‚úÖ Technical Skills Practiced from Pandas_02:\n",
    "- **Identified and documented data quality issues** using systematic exploration methods\n",
    "- **Applied pandas string methods** (.str.title(), .str.replace(), .fillna()) for text standardization\n",
    "- **Handled missing data** with appropriate strategies for cultural research contexts\n",
    "- **Created clean, analysis-ready datasets** through systematic workflows\n",
    "- **Performed exploratory data analysis** to reveal cultural patterns\n",
    "- **Created meaningful visualizations** that communicate cultural insights effectively\n",
    "\n",
    "### ‚úÖ Cultural Research Skills Developed:\n",
    "- **Applied ethical data collection principles** including robots.txt compliance and AI-era considerations\n",
    "- **Critical evaluation of data quality and bias** in cultural contexts\n",
    "- **Understanding the impact of data cleaning choices** on cultural analysis outcomes\n",
    "- **Recognition of patterns in cultural datasets** and their broader significance\n",
    "- **Appreciation for the complexity** of real-world cultural data and its limitations\n",
    "\n",
    "### \udf10 Contemporary Digital Humanities Skills:\n",
    "- **Ethical data stewardship** in an era of AI training and large-scale data collection\n",
    "- **Cultural sensitivity** in data standardization and categorization processes\n",
    "- **Critical assessment** of whose voices are represented (and missing) in cultural datasets\n",
    "- **Responsible attribution** and source documentation practices\n",
    "\n",
    "### üéì Next Steps in Your Cultural Data Journey:\n",
    "1. **Expand your toolkit**: Learn advanced pandas techniques like merging datasets and time series analysis\n",
    "2. **Deepen domain expertise**: Explore specialized tools for your specific cultural research interests\n",
    "3. **Build ethical practices**: Develop frameworks for responsible cultural data collection and analysis\n",
    "4. **Share responsibly**: Present findings with appropriate context about limitations and biases\n",
    "5. **Contribute to the field**: Engage with digital humanities communities and contemporary debates about data ethics\n",
    "\n",
    "### üîó Connection to Course Goals:\n",
    "The data cleaning skills you've practiced here directly support:\n",
    "- **Critical digital literacy**: Understanding how data collection and processing choices shape cultural narratives\n",
    "- **Ethical research practices**: Applying contemporary standards for responsible data use\n",
    "- **Technical proficiency**: Building computational skills for cultural analysis\n",
    "- **Cultural insight**: Using quantitative methods to deepen understanding of cultural phenomena\n",
    "\n",
    "Remember: **Clean data is the foundation of trustworthy cultural analysis, but the choices we make in cleaning reflect our values and responsibilities as cultural researchers.** The skills you've practiced here‚Äîboth technical and ethical‚Äîwill serve you well in any data-driven cultural research project!\n",
    "\n",
    "### üìã Submit Your Work:\n",
    "Ensure your notebook includes:\n",
    "- [ ] **Completed ethics reflection** with thoughtful consideration of data sources and biases\n",
    "- [ ] **Working code** applied to your actual cultural dataset\n",
    "- [ ] **Clear documentation** of your data cleaning decisions and their rationale\n",
    "- [ ] **Meaningful visualizations** that reveal cultural patterns\n",
    "- [ ] **Critical analysis** connecting your findings to broader cultural questions\n",
    "- [ ] **Reflection on limitations** and future research directions\n",
    "- [ ] **Proper attribution** of data sources and acknowledgment of ethical considerations"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
